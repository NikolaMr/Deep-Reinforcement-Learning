{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "from skimage import color, exposure, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 14:15:29,008] Making new env: PongDeterministic-v4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = env.action_space.n # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 500. # timesteps to observe before training\n",
    "EXPLORE = 500000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.1 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "REPLAY_MEMORY = 30000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "LEARNING_RATE = 1e-5\n",
    "SGD_LEARNING_RATE = 1e-3\n",
    "\n",
    "img_rows , img_cols = 84, 84\n",
    "#Convert image into Black and white\n",
    "img_channels = 3 #We stack 3 frames\n",
    "\n",
    "max_epLength = 1000\n",
    "\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "SAVE_DIR = 'dqn_pong/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='valid',input_shape=(img_rows,img_cols,img_channels)))  #80*80*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS, activation='linear'))\n",
    "\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    #sgd = SGD(lr=SGD_LEARNING_RATE)\n",
    "    #model.compile(loss='mse',optimizer=sgd)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, io\n",
    "from matplotlib import pyplot as plt\n",
    "    \n",
    "def show_as_img(arr):\n",
    "    io.imshow(arr.reshape(img_rows, img_cols, 3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, buff_sz):\n",
    "        self.buff_sz = buff_sz\n",
    "        self.M = deque()\n",
    "    def append(self, tup):\n",
    "        self.M.append(tup)\n",
    "        if (len(self.M) > self.buff_sz):\n",
    "            dump = self.M.popleft()\n",
    "            if dump[2] > 0.0:\n",
    "                if random.random() < 0.65:\n",
    "                    self.append(dump)\n",
    "            elif dump[2] < 0.0:\n",
    "                if random.random() < 0.45:\n",
    "                    self.append(dump)\n",
    "    def sample(self, num_samples):\n",
    "        minibatch = random.sample(self.M, num_samples)\n",
    "        return minibatch\n",
    "        #indices_random = random.randrange(0, len(self.M) - num_samples)\n",
    "        #return list(self.M)[indices_random:indices_random + num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_cols,img_rows), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_cols, img_rows, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TIMESTEP = 0\n",
    "\n",
    "def train_model(model, env):\n",
    "    \n",
    "    M = Memory(REPLAY_MEMORY)\n",
    " \n",
    "    OBSERVE = OBSERVATION\n",
    "    epsilon = INITIAL_EPSILON\n",
    "\n",
    "    t = 0\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for idxEpisode in range(NUM_EPISODES):\n",
    "        #Reset environment and get first new observation\n",
    "        x_t = env.reset()\n",
    "        x_t = process_frame(x_t)\n",
    "        s_t = np.stack((x_t, x_t, x_t), axis=3)\n",
    "        s_t = s_t.reshape(1, s_t.shape[1], s_t.shape[2], s_t.shape[3])\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        loss = 0.0\n",
    "        ct_non_zero_reward = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            a_t = None\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < epsilon or t < OBSERVE:\n",
    "                a_t = random.randrange(ACTIONS)\n",
    "            else:\n",
    "                q = model.predict(s_t)\n",
    "                policy_max_Q = np.argmax(q)\n",
    "                a_t = policy_max_Q\n",
    "            x_t1,r_t,done,_ = env.step(a_t)\n",
    "            x_t1 = process_frame(x_t1)\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :2], axis=3)\n",
    "            \n",
    "            t += 1\n",
    "            TIMESTEP = t\n",
    "            M.append((s_t, a_t, r_t, s_t1, done))\n",
    "            \n",
    "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "                minibatch = M.sample(BATCH)\n",
    "                inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
    "                targets = np.zeros((BATCH, ACTIONS))\n",
    "                for i in range(0, BATCH):\n",
    "                    state_t = minibatch[i][0]\n",
    "                    action_t = minibatch[i][1]\n",
    "                    reward_t = minibatch[i][2]\n",
    "                    state_t1 = minibatch[i][3]\n",
    "                    done_t = minibatch[i][4]\n",
    "\n",
    "                    inputs[i] = state_t\n",
    "                    targets[i] = model.predict(state_t)\n",
    "                    Q_sa = model.predict(state_t1)\n",
    "                    if done_t:\n",
    "                        #print('targets before done', targets)\n",
    "                        targets[i, action_t] = reward_t\n",
    "                        #print('targets after done', targets)\n",
    "                    else:\n",
    "                        targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa[0])#[action_t]\n",
    "                    if reward_t != 0.0:\n",
    "                        ct_non_zero_reward += 1\n",
    "                        #print('got reward', reward_t, 'for action', action_t)\n",
    "                        #print('bef', model.predict(state_t))\n",
    "                        #print('pred_s1', model.predict(state_t1))\n",
    "                        #print('aft', targets)\n",
    "                loss += model.train_on_batch(inputs, targets)\n",
    "            rAll += r_t\n",
    "            s_t = s_t1\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "            \n",
    "        rewards.append(rAll)\n",
    "        \n",
    "        print('episode', idxEpisode, 'length', j, 'reward', rAll, 'epsilon', epsilon, 'loss sum', loss, 'non zero rewards', ct_non_zero_reward)\n",
    "        \n",
    "        if idxEpisode % 50 == 0:\n",
    "            path = SAVE_DIR + 'model_episode_' + str(idxEpisode) + '.h5'\n",
    "            save_model(model, path)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = build_model()\n",
    "model = keras.models.load_model('/home/nikola/Faks/Diplomski/TreciSemestar/Projekt/atari_player/dqn_pong/model_episode_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.lr = SGD_LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 length 979 reward -20.0 epsilon 0.499616799999989 loss sum 0.295788373158 non zero rewards 358\n",
      "episode 1 length 1000 reward -13.0 epsilon 0.498816799999966 loss sum 1.2070716663 non zero rewards 616\n",
      "episode 2 length 1000 reward -11.0 epsilon 0.49801679999994297 loss sum 1.42859140283 non zero rewards 579\n",
      "episode 3 length 1000 reward -11.0 epsilon 0.49721679999991997 loss sum 1.22577359623 non zero rewards 517\n",
      "episode 4 length 1000 reward -13.0 epsilon 0.49641679999989696 loss sum 1.15177312976 non zero rewards 537\n",
      "episode 5 length 1000 reward -5.0 epsilon 0.49561679999987396 loss sum 1.21708032858 non zero rewards 542\n",
      "episode 6 length 1000 reward -11.0 epsilon 0.49481679999985095 loss sum 1.30311307564 non zero rewards 542\n",
      "episode 7 length 1000 reward -6.0 epsilon 0.49401679999982795 loss sum 1.18956770236 non zero rewards 508\n",
      "episode 8 length 1000 reward -3.0 epsilon 0.49321679999980494 loss sum 1.25695371279 non zero rewards 507\n",
      "episode 9 length 1000 reward -12.0 epsilon 0.49241679999978194 loss sum 1.29857096593 non zero rewards 457\n",
      "episode 10 length 1000 reward -9.0 epsilon 0.49161679999975894 loss sum 1.43598380461 non zero rewards 481\n",
      "episode 11 length 1000 reward -9.0 epsilon 0.49081679999973593 loss sum 1.401480992 non zero rewards 508\n",
      "episode 12 length 1000 reward -9.0 epsilon 0.4900167999997129 loss sum 1.39450272256 non zero rewards 463\n",
      "episode 13 length 1000 reward -12.0 epsilon 0.4892167999996899 loss sum 1.30171356144 non zero rewards 460\n",
      "episode 14 length 1000 reward -13.0 epsilon 0.4884167999996669 loss sum 1.2458130169 non zero rewards 480\n",
      "episode 15 length 1000 reward -10.0 epsilon 0.4876167999996439 loss sum 1.37246317108 non zero rewards 488\n",
      "episode 16 length 1000 reward -6.0 epsilon 0.4868167999996209 loss sum 1.31097333785 non zero rewards 486\n",
      "episode 17 length 1000 reward -11.0 epsilon 0.4860167999995979 loss sum 1.28551328466 non zero rewards 461\n",
      "episode 18 length 1000 reward -14.0 epsilon 0.4852167999995749 loss sum 1.23879199818 non zero rewards 440\n",
      "episode 19 length 1000 reward -10.0 epsilon 0.4844167999995519 loss sum 1.29464437236 non zero rewards 474\n",
      "episode 20 length 1000 reward -7.0 epsilon 0.4836167999995289 loss sum 1.17890033993 non zero rewards 441\n",
      "episode 21 length 1000 reward -5.0 epsilon 0.4828167999995059 loss sum 1.25584259522 non zero rewards 461\n",
      "episode 22 length 1000 reward -14.0 epsilon 0.4820167999994829 loss sum 1.25752170278 non zero rewards 444\n",
      "episode 23 length 1000 reward -14.0 epsilon 0.4812167999994599 loss sum 1.25454191712 non zero rewards 500\n",
      "episode 24 length 1000 reward -9.0 epsilon 0.48041679999943687 loss sum 1.22201799092 non zero rewards 476\n",
      "episode 25 length 1000 reward -17.0 epsilon 0.47961679999941387 loss sum 1.23853367052 non zero rewards 467\n",
      "episode 26 length 1000 reward -3.0 epsilon 0.47881679999939086 loss sum 1.23854816271 non zero rewards 477\n",
      "episode 27 length 1000 reward -11.0 epsilon 0.47801679999936786 loss sum 1.24033200278 non zero rewards 429\n",
      "episode 28 length 1000 reward -5.0 epsilon 0.47721679999934485 loss sum 1.26509074797 non zero rewards 453\n",
      "episode 29 length 1000 reward -10.0 epsilon 0.47641679999932185 loss sum 1.3139465415 non zero rewards 490\n",
      "episode 30 length 1000 reward -12.0 epsilon 0.47561679999929884 loss sum 1.42490933105 non zero rewards 439\n",
      "episode 31 length 1000 reward -8.0 epsilon 0.47481679999927584 loss sum 1.3101493821 non zero rewards 457\n",
      "episode 32 length 1000 reward -11.0 epsilon 0.47401679999925284 loss sum 1.26204235022 non zero rewards 472\n",
      "episode 33 length 1000 reward -14.0 epsilon 0.47321679999922983 loss sum 1.24941177719 non zero rewards 470\n",
      "episode 34 length 1000 reward -8.0 epsilon 0.4724167999992068 loss sum 1.33326593557 non zero rewards 501\n",
      "episode 35 length 1000 reward -17.0 epsilon 0.4716167999991838 loss sum 1.26123408836 non zero rewards 479\n",
      "episode 36 length 1000 reward -11.0 epsilon 0.4708167999991608 loss sum 1.35058856742 non zero rewards 519\n",
      "episode 37 length 1000 reward -13.0 epsilon 0.4700167999991378 loss sum 1.3071198875 non zero rewards 587\n",
      "episode 38 length 1000 reward -14.0 epsilon 0.4692167999991148 loss sum 1.29424591095 non zero rewards 544\n",
      "episode 39 length 1000 reward -8.0 epsilon 0.4684167999990918 loss sum 1.29140827977 non zero rewards 532\n",
      "episode 40 length 1000 reward -15.0 epsilon 0.4676167999990688 loss sum 1.29972475203 non zero rewards 547\n",
      "episode 41 length 1000 reward -11.0 epsilon 0.4668167999990458 loss sum 1.34484571993 non zero rewards 565\n",
      "episode 42 length 1000 reward -7.0 epsilon 0.4660167999990228 loss sum 1.26887014549 non zero rewards 549\n",
      "episode 43 length 1000 reward -9.0 epsilon 0.4652167999989998 loss sum 1.34125777168 non zero rewards 596\n",
      "episode 44 length 1000 reward -9.0 epsilon 0.4644167999989768 loss sum 1.34075057216 non zero rewards 562\n",
      "episode 45 length 1000 reward -7.0 epsilon 0.4636167999989538 loss sum 1.34142215119 non zero rewards 587\n",
      "episode 46 length 1000 reward -13.0 epsilon 0.4628167999989308 loss sum 1.34130745735 non zero rewards 568\n",
      "episode 47 length 1000 reward -10.0 epsilon 0.46201679999890777 loss sum 1.32898665237 non zero rewards 591\n",
      "episode 48 length 1000 reward -10.0 epsilon 0.46121679999888476 loss sum 1.32318332483 non zero rewards 632\n",
      "episode 49 length 1000 reward -6.0 epsilon 0.46041679999886176 loss sum 1.32636187706 non zero rewards 621\n",
      "episode 50 length 1000 reward -14.0 epsilon 0.45961679999883875 loss sum 1.42477647975 non zero rewards 629\n",
      "episode 51 length 1000 reward -10.0 epsilon 0.45881679999881575 loss sum 1.3923475265 non zero rewards 639\n",
      "episode 52 length 1000 reward -6.0 epsilon 0.45801679999879275 loss sum 1.36728714354 non zero rewards 681\n",
      "episode 53 length 1000 reward -11.0 epsilon 0.45721679999876974 loss sum 1.3469584657 non zero rewards 654\n",
      "episode 54 length 1000 reward -10.0 epsilon 0.45641679999874674 loss sum 1.37298863928 non zero rewards 683\n",
      "episode 55 length 1000 reward -11.0 epsilon 0.45561679999872373 loss sum 1.43862365148 non zero rewards 690\n",
      "episode 56 length 1000 reward -11.0 epsilon 0.4548167999987007 loss sum 1.38792788534 non zero rewards 679\n",
      "episode 57 length 1000 reward -10.0 epsilon 0.4540167999986777 loss sum 1.33279653885 non zero rewards 687\n",
      "episode 58 length 1000 reward -10.0 epsilon 0.4532167999986547 loss sum 1.43439705126 non zero rewards 704\n",
      "episode 59 length 1000 reward -9.0 epsilon 0.4524167999986317 loss sum 1.42460328739 non zero rewards 701\n",
      "episode 60 length 1000 reward -10.0 epsilon 0.4516167999986087 loss sum 1.42568223369 non zero rewards 717\n",
      "episode 61 length 1000 reward -14.0 epsilon 0.4508167999985857 loss sum 1.41505971097 non zero rewards 726\n",
      "episode 62 length 1000 reward -12.0 epsilon 0.4500167999985627 loss sum 1.42654253382 non zero rewards 739\n",
      "episode 63 length 1000 reward -7.0 epsilon 0.4492167999985397 loss sum 1.37441692213 non zero rewards 675\n",
      "episode 64 length 1000 reward -8.0 epsilon 0.4484167999985167 loss sum 1.38573151943 non zero rewards 681\n",
      "episode 65 length 1000 reward -4.0 epsilon 0.4476167999984937 loss sum 1.39174215935 non zero rewards 700\n",
      "episode 66 length 1000 reward -10.0 epsilon 0.4468167999984707 loss sum 1.38151915449 non zero rewards 707\n",
      "episode 67 length 1000 reward -11.0 epsilon 0.4460167999984477 loss sum 1.35665352922 non zero rewards 764\n",
      "episode 68 length 1000 reward -12.0 epsilon 0.4452167999984247 loss sum 1.37942915238 non zero rewards 735\n",
      "episode 69 length 1000 reward -11.0 epsilon 0.44441679999840167 loss sum 1.39697213977 non zero rewards 733\n",
      "episode 70 length 1000 reward -8.0 epsilon 0.44361679999837866 loss sum 1.40768047312 non zero rewards 712\n",
      "episode 71 length 1000 reward -7.0 epsilon 0.44281679999835566 loss sum 1.40357786338 non zero rewards 715\n",
      "episode 72 length 1000 reward -13.0 epsilon 0.44201679999833265 loss sum 1.36338163548 non zero rewards 736\n",
      "episode 73 length 1000 reward -7.0 epsilon 0.44121679999830965 loss sum 1.36543335993 non zero rewards 712\n",
      "episode 74 length 1000 reward -6.0 epsilon 0.44041679999828665 loss sum 1.48736892617 non zero rewards 746\n",
      "episode 75 length 1000 reward -5.0 epsilon 0.43961679999826364 loss sum 1.30662664483 non zero rewards 657\n",
      "episode 76 length 1000 reward -3.0 epsilon 0.43881679999824064 loss sum 1.4060244507 non zero rewards 786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 77 length 1000 reward -8.0 epsilon 0.43801679999821763 loss sum 1.42982262257 non zero rewards 675\n",
      "episode 78 length 1000 reward -6.0 epsilon 0.4372167999981946 loss sum 1.35252449315 non zero rewards 777\n",
      "episode 79 length 1000 reward -9.0 epsilon 0.4364167999981716 loss sum 1.39648033315 non zero rewards 737\n",
      "episode 80 length 1000 reward -9.0 epsilon 0.4356167999981486 loss sum 1.44307669249 non zero rewards 761\n",
      "episode 81 length 1000 reward -11.0 epsilon 0.4348167999981256 loss sum 1.41913116566 non zero rewards 751\n",
      "episode 82 length 1000 reward -7.0 epsilon 0.4340167999981026 loss sum 1.3946281377 non zero rewards 761\n",
      "episode 83 length 1000 reward -4.0 epsilon 0.4332167999980796 loss sum 1.44602089892 non zero rewards 735\n",
      "episode 84 length 1000 reward -11.0 epsilon 0.4324167999980566 loss sum 1.42109691855 non zero rewards 761\n",
      "episode 85 length 1000 reward -16.0 epsilon 0.4316167999980336 loss sum 1.41586953076 non zero rewards 756\n",
      "episode 86 length 1000 reward -11.0 epsilon 0.4308167999980106 loss sum 1.48036760266 non zero rewards 753\n",
      "episode 87 length 1000 reward -5.0 epsilon 0.4300167999979876 loss sum 1.44603066237 non zero rewards 798\n",
      "episode 88 length 1000 reward -8.0 epsilon 0.4292167999979646 loss sum 1.43484656123 non zero rewards 778\n",
      "episode 89 length 1000 reward -8.0 epsilon 0.4284167999979416 loss sum 1.35376390527 non zero rewards 727\n",
      "episode 90 length 1000 reward -7.0 epsilon 0.4276167999979186 loss sum 1.4414050327 non zero rewards 778\n",
      "episode 91 length 1000 reward -13.0 epsilon 0.42681679999789557 loss sum 1.40806895468 non zero rewards 796\n",
      "episode 92 length 1000 reward -4.0 epsilon 0.42601679999787256 loss sum 1.43936180684 non zero rewards 721\n",
      "episode 93 length 1000 reward -8.0 epsilon 0.42521679999784956 loss sum 1.5155416873 non zero rewards 740\n",
      "episode 94 length 1000 reward -9.0 epsilon 0.42441679999782655 loss sum 1.43286590837 non zero rewards 730\n",
      "episode 95 length 1000 reward -8.0 epsilon 0.42361679999780355 loss sum 1.43934615186 non zero rewards 759\n",
      "episode 96 length 1000 reward -7.0 epsilon 0.42281679999778055 loss sum 1.50059901513 non zero rewards 782\n",
      "episode 97 length 1000 reward -16.0 epsilon 0.42201679999775754 loss sum 1.48133074891 non zero rewards 773\n",
      "episode 98 length 1000 reward -10.0 epsilon 0.42121679999773454 loss sum 1.50247881486 non zero rewards 774\n"
     ]
    }
   ],
   "source": [
    "rewards = train_model(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(len(rewards)), rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model(model, Wrapped_Game(env))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
