{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "from skimage import color, exposure, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 15:17:26,980] Making new env: PongDeterministic-v4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = env.action_space.n # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 30000. # timesteps to observe before training\n",
    "EXPLORE = 700000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "INITIAL_EPSILON = 1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 30000 # number of previous transitions to remember\n",
    "BATCH = 64 # size of minibatch\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "img_rows , img_cols = 80, 80\n",
    "#Convert image into Black and white\n",
    "img_channels = 3 #We stack 3 frames\n",
    "\n",
    "max_epLength = 3000\n",
    "\n",
    "update_freq = 4\n",
    "\n",
    "NUM_EPISODES = 5000\n",
    "\n",
    "MODEL_NAME = \"breakout_v4_model.h5\"\n",
    "\n",
    "tau = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    initializer = initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(img_rows,img_cols,img_channels), kernel_initializer=initializer, bias_initializer='zeros'))  #80*80*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same', kernel_initializer=initializer, bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same', kernel_initializer=initializer, bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_initializer=initializer, bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS, kernel_initializer=initializer, bias_initializer='zeros'))\n",
    "\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapped_Game:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.game.reset()\n",
    "    def step(self, action):\n",
    "        ns, r, d, _ = self.game.step(action)\n",
    "        if d:\n",
    "            self.game.reset()\n",
    "        return ns, r, d, _\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "    def render(self):\n",
    "        self.game.render()\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model):\n",
    "    \"\"\"Returns a copy of a keras model.\"\"\"\n",
    "    model.save('tmp_model')\n",
    "    return keras.models.load_model('tmp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_session(model, target_model):\n",
    "    from keras import backend as K\n",
    "    model_path = 'tmp_model_name_ddqn'\n",
    "    model.save(model_path)\n",
    "    del model\n",
    "    target_model_path = 'tmp_target_model_name_ddqn'\n",
    "    target_model.save(target_model_path)\n",
    "    del target_model\n",
    "    K.clear_session()\n",
    "    model = keras.models.load_model(model_path)\n",
    "    target_model = keras.models.load_model(target_model_path)\n",
    "    return model, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_linear_comb(m, tm, tau):\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    '''Sets the value of a tensor variable,\n",
    "    from a Numpy array.\n",
    "    '''\n",
    "    #tf.assign(x, np.asarray(value)).op.run(session=get_session())\n",
    "    assign_op = tm.assign(m.value() * tau + (1-tau) * tm.value())\n",
    "    #K.get_session().run(assign_op, feed_dict={assign_placeholder: value})\n",
    "    return assign_op\n",
    "\n",
    "def update_target_graph(target_model, model, tau):\n",
    "    var_assign_ops = []\n",
    "    for idxLayer in range(len(model.layers)):\n",
    "        model_layer = model.layers[idxLayer]\n",
    "        target_model_layer = target_model.layers[idxLayer]\n",
    "        for idxWeight in range(len(model_layer.weights)):\n",
    "            var_assign_ops.append(\n",
    "                assign_linear_comb(model_layer.weights[idxWeight], target_model_layer.weights[idxWeight], tau)\n",
    "            )\n",
    "    return var_assign_ops\n",
    "    \n",
    "def update_target(var_assign_ops):\n",
    "    from keras import backend as K\n",
    "    for var_assign_op in var_assign_ops:\n",
    "        K.get_session().run(var_assign_op)\n",
    "    \"\"\"for idxLayer in range(len(model.layers)):\n",
    "        model_layer = model.layers[idxLayer]\n",
    "        target_model_layer = target_model.layers[idxLayer]\n",
    "        for i, model_weight, target_model_weight in zip(range(len(model_layer.weights)), model_layer.get_weights(), target_model_layer.get_weights()):\n",
    "            new_weight = tau * model_weight + (1 - tau) * target_model_weight\n",
    "            set_value(target_model_layer.weights[i], new_weight)\n",
    "            \"\"\"\n",
    "#target_model = copy_model(model)\n",
    "#var_assign_ops = update_target_graph(target_model, model, tau)\n",
    "#update_target(var_assign_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TIMESTEP = 0\n",
    "\n",
    "def train_model(model, env):\n",
    "    \n",
    "    target_model = build_model()\n",
    "    var_assign_ops = update_target_graph(target_model, model, tau)\n",
    "    \n",
    "    #init replay memory\n",
    "    M = deque()\n",
    " \n",
    "    OBSERVE = OBSERVATION\n",
    "    epsilon = INITIAL_EPSILON\n",
    "\n",
    "    t = 0\n",
    "    \n",
    "    for idxEpisode in range(NUM_EPISODES):\n",
    "        #Reset environment and get first new observation\n",
    "        x_t = env.reset()\n",
    "        x_t = skimage.color.rgb2gray(x_t)\n",
    "        x_t = skimage.transform.resize(x_t,(img_rows,img_cols))\n",
    "        x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "        x_t = x_t.reshape((img_rows, img_cols, 1))\n",
    "        s_t = np.stack((x_t, x_t, x_t), axis=2)\n",
    "        s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            a_t = None\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < epsilon or t < OBSERVE:\n",
    "                a_t = random.randrange(ACTIONS)\n",
    "            else:\n",
    "                q = model.predict(s_t)\n",
    "                policy_max_Q = np.argmax(q)\n",
    "                a_t = policy_max_Q\n",
    "            x_t1,r_t,done,_ = env.step(a_t)\n",
    "            x_t1 = skimage.color.rgb2gray(x_t1)\n",
    "            x_t1 = skimage.transform.resize(x_t1,(img_cols,img_rows))\n",
    "            x_t1 = skimage.exposure.rescale_intensity(x_t1,out_range=(0,255))\n",
    "            x_t1 = x_t1.reshape((1, img_cols, img_rows, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :2], axis=3)\n",
    "            \n",
    "            t += 1\n",
    "            TIMESTEP = t\n",
    "            M.append((s_t, a_t, r_t, s_t1, done))\n",
    "            if (len(M) > REPLAY_MEMORY):\n",
    "                M.popleft()\n",
    "            \n",
    "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "                \n",
    "                if t % (update_freq) == 0:\n",
    "                    minibatch = random.sample(M, BATCH)\n",
    "                    inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
    "                    targets = np.zeros((BATCH, ACTIONS))\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    #Q1 = model.predict(s_t1)\n",
    "                    #Q2 = target_model.predict(s_t1)\n",
    "                    #end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    #doubleQ = Q2[range(batch_size),Q1]\n",
    "                    #targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    #_ = sess.run(mainQN.updateModel, \\\n",
    "                    #    feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    # experience replay\n",
    "                    for i in range(0, BATCH):\n",
    "                        state_t = minibatch[i][0]\n",
    "                        action_t = minibatch[i][1]\n",
    "                        reward_t = minibatch[i][2]\n",
    "                        state_t1 = minibatch[i][3]\n",
    "                        done_t = minibatch[i][4]\n",
    "\n",
    "                        inputs[i] = state_t\n",
    "                        #print (inputs[i])\n",
    "                        #print (state_t)\n",
    "                        targets[i] = model.predict(state_t)\n",
    "                        # DDQN formula\n",
    "                        # Q-Target = r + γQ(s’,argmax(Q(s’,a,ϴ),ϴ’))\n",
    "                        #print(targets[i].shape)\n",
    "                        Q_sa = target_model.predict(state_t1)\n",
    "                        if done_t:\n",
    "                            targets[i, action_t] = reward_t\n",
    "                        else:\n",
    "                            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa[0])#[action_t]\n",
    "\n",
    "                    model.train_on_batch(inputs, targets)\n",
    "                    time1 = time.time()\n",
    "                    update_target(var_assign_ops)\n",
    "                    time2 = time.time()\n",
    "                    #if t % 5 == 0:\n",
    "                    #    print('%s function took %0.3f ms at iteration %d' % ('updateTarget', (time2-time1)*1000.0, t))\n",
    "                    #updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r_t\n",
    "            s_t = s_t1\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "            \n",
    "            #if t % 10000:\n",
    "            #    clear_session(model, target_model)\n",
    "        print('episode', idxEpisode, 'length', j, 'reward', rAll, 'epsilon', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finish building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), bias_initializer=\"zeros\", padding=\"same\", input_shape=(80, 80, 3..., strides=(4, 4), kernel_initializer=<keras.ini...)`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), bias_initializer=\"zeros\", padding=\"same\", strides=(2, 2), kernel_initializer=<keras.ini...)`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), bias_initializer=\"zeros\", padding=\"same\", strides=(1, 1), kernel_initializer=<keras.ini...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "#model.load_weights('1100000_iters_breakout_deterministic_model_atari_ddqn_more_explore.h5')\n",
    "#model.load_weights('backup_pong_lr-4/250000_iters_pdv4_ddqn_lr-4_tmr_100_after_500000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), bias_initializer=\"zeros\", padding=\"same\", input_shape=(80, 80, 3..., strides=(4, 4), kernel_initializer=<keras.ini...)`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), bias_initializer=\"zeros\", padding=\"same\", strides=(2, 2), kernel_initializer=<keras.ini...)`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), bias_initializer=\"zeros\", padding=\"same\", strides=(1, 1), kernel_initializer=<keras.ini...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finish building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 length 978 reward -20.0 epsilon 1\n",
      "episode 1 length 903 reward -20.0 epsilon 1\n",
      "episode 2 length 764 reward -21.0 epsilon 1\n",
      "episode 3 length 824 reward -21.0 epsilon 1\n",
      "episode 4 length 1037 reward -19.0 epsilon 1\n",
      "episode 5 length 764 reward -21.0 epsilon 1\n",
      "episode 6 length 871 reward -21.0 epsilon 1\n",
      "episode 7 length 881 reward -21.0 epsilon 1\n",
      "episode 8 length 1069 reward -20.0 epsilon 1\n",
      "episode 9 length 764 reward -21.0 epsilon 1\n",
      "episode 10 length 824 reward -21.0 epsilon 1\n",
      "episode 11 length 1067 reward -20.0 epsilon 1\n",
      "episode 12 length 842 reward -20.0 epsilon 1\n",
      "episode 13 length 964 reward -20.0 epsilon 1\n",
      "episode 14 length 949 reward -20.0 epsilon 1\n",
      "episode 15 length 839 reward -21.0 epsilon 1\n",
      "episode 16 length 852 reward -21.0 epsilon 1\n",
      "episode 17 length 884 reward -21.0 epsilon 1\n",
      "episode 18 length 792 reward -21.0 epsilon 1\n",
      "episode 19 length 764 reward -21.0 epsilon 1\n",
      "episode 20 length 886 reward -21.0 epsilon 1\n",
      "episode 21 length 1069 reward -20.0 epsilon 1\n",
      "episode 22 length 844 reward -21.0 epsilon 1\n",
      "episode 23 length 943 reward -21.0 epsilon 1\n",
      "episode 24 length 852 reward -21.0 epsilon 1\n",
      "episode 25 length 947 reward -21.0 epsilon 1\n",
      "episode 26 length 1009 reward -20.0 epsilon 1\n",
      "episode 27 length 922 reward -20.0 epsilon 1\n",
      "episode 28 length 822 reward -21.0 epsilon 1\n",
      "episode 29 length 1293 reward -19.0 epsilon 1\n",
      "episode 30 length 860 reward -20.0 epsilon 1\n",
      "episode 31 length 820 reward -21.0 epsilon 1\n",
      "episode 32 length 842 reward -20.0 epsilon 1\n",
      "episode 33 length 811 reward -21.0 epsilon 0.9992193142856962\n",
      "episode 34 length 843 reward -21.0 epsilon 0.9980270714285258\n",
      "episode 35 length 824 reward -21.0 epsilon 0.9968616999999274\n",
      "episode 36 length 1020 reward -19.0 epsilon 0.9954191285713225\n",
      "episode 37 length 944 reward -21.0 epsilon 0.9940840428570059\n",
      "episode 38 length 824 reward -21.0 epsilon 0.9929186714284075\n",
      "episode 39 length 764 reward -21.0 epsilon 0.9918381571426682\n",
      "episode 40 length 901 reward -20.0 epsilon 0.9905638857140673\n",
      "episode 41 length 900 reward -20.0 epsilon 0.9892910285711807\n",
      "episode 42 length 1110 reward -20.0 epsilon 0.9877211714282872\n",
      "episode 43 length 886 reward -21.0 epsilon 0.986468114285401\n",
      "episode 44 length 910 reward -20.0 epsilon 0.9851811142853713\n",
      "episode 45 length 912 reward -21.0 epsilon 0.9838912857139128\n",
      "episode 46 length 840 reward -21.0 epsilon 0.9827032857138853\n",
      "episode 47 length 928 reward -20.0 epsilon 0.9813908285709978\n",
      "episode 48 length 1189 reward -17.0 epsilon 0.9797092428566732\n",
      "episode 49 length 793 reward -21.0 epsilon 0.9785877142852186\n",
      "episode 50 length 852 reward -21.0 epsilon 0.9773827428566193\n",
      "episode 51 length 999 reward -21.0 epsilon 0.9759698714280152\n",
      "episode 52 length 870 reward -20.0 epsilon 0.9747394428565581\n",
      "episode 53 length 898 reward -20.0 epsilon 0.9734694142851001\n",
      "episode 54 length 991 reward -20.0 epsilon 0.9720678571422106\n",
      "episode 55 length 1054 reward -20.0 epsilon 0.9705771999993189\n",
      "episode 56 length 822 reward -21.0 epsilon 0.9694146571421491\n",
      "episode 57 length 1004 reward -19.0 epsilon 0.9679947142849734\n",
      "episode 58 length 842 reward -20.0 epsilon 0.9668038857135173\n",
      "episode 59 length 914 reward -21.0 epsilon 0.9655112285706302\n",
      "episode 60 length 1243 reward -19.0 epsilon 0.9637532714277324\n",
      "episode 61 length 1083 reward -19.0 epsilon 0.9622215999991255\n",
      "episode 62 length 1017 reward -20.0 epsilon 0.9607832714276636\n",
      "episode 63 length 1039 reward -20.0 epsilon 0.9593138285704867\n",
      "episode 64 length 1129 reward -19.0 epsilon 0.9577170999990212\n",
      "episode 65 length 947 reward -20.0 epsilon 0.9563777714275616\n",
      "episode 66 length 837 reward -20.0 epsilon 0.9551940142846771\n",
      "episode 67 length 911 reward -21.0 epsilon 0.953905599998933\n",
      "episode 68 length 872 reward -21.0 epsilon 0.9526723428560473\n",
      "episode 69 length 969 reward -21.0 epsilon 0.9513018999988727\n",
      "episode 70 length 990 reward -20.0 epsilon 0.9499017571416974\n",
      "episode 71 length 1184 reward -17.0 epsilon 0.9482272428559444\n",
      "episode 72 length 931 reward -20.0 epsilon 0.9469105428559139\n",
      "episode 73 length 890 reward -20.0 epsilon 0.9456518285701705\n",
      "episode 74 length 920 reward -20.0 epsilon 0.9443506857129975\n",
      "episode 75 length 990 reward -19.0 epsilon 0.9429505428558222\n",
      "episode 76 length 941 reward -20.0 epsilon 0.9416196999986486\n",
      "episode 77 length 783 reward -21.0 epsilon 0.9405123142843372\n",
      "episode 78 length 764 reward -21.0 epsilon 0.9394317999985979\n",
      "episode 79 length 1047 reward -19.0 epsilon 0.9379510428557065\n",
      "episode 80 length 832 reward -21.0 epsilon 0.9367743571413936\n",
      "episode 81 length 764 reward -21.0 epsilon 0.9356938428556543\n",
      "episode 82 length 824 reward -21.0 epsilon 0.9345284714270559\n",
      "episode 83 length 870 reward -20.0 epsilon 0.9332980428555988\n",
      "episode 84 length 884 reward -21.0 epsilon 0.9320478142841413\n",
      "episode 85 length 783 reward -21.0 epsilon 0.93094042856983\n",
      "episode 86 length 942 reward -21.0 epsilon 0.929608171426942\n",
      "episode 87 length 854 reward -21.0 epsilon 0.928400371426914\n",
      "episode 88 length 870 reward -20.0 epsilon 0.927169942855457\n",
      "episode 89 length 1081 reward -20.0 epsilon 0.9256410999982787\n",
      "episode 90 length 853 reward -21.0 epsilon 0.9244347142839651\n",
      "episode 91 length 1080 reward -21.0 epsilon 0.9229072857125011\n",
      "episode 92 length 886 reward -21.0 epsilon 0.921654228569615\n",
      "episode 93 length 920 reward -21.0 epsilon 0.920353085712442\n",
      "episode 94 length 843 reward -21.0 epsilon 0.9191608428552716\n",
      "episode 95 length 1141 reward -19.0 epsilon 0.9175471428552342\n",
      "episode 96 length 962 reward -20.0 epsilon 0.9161865999980598\n",
      "episode 97 length 824 reward -21.0 epsilon 0.9150212285694614\n",
      "episode 98 length 824 reward -21.0 epsilon 0.913855857140863\n",
      "episode 99 length 824 reward -21.0 epsilon 0.9126904857122646\n",
      "episode 100 length 950 reward -20.0 epsilon 0.9113469142836621\n",
      "episode 101 length 942 reward -21.0 epsilon 0.9100146571407741\n",
      "episode 102 length 1089 reward -18.0 epsilon 0.9084744999978813\n",
      "episode 103 length 854 reward -21.0 epsilon 0.9072666999978534\n",
      "episode 104 length 964 reward -20.0 epsilon 0.9059033285692504\n",
      "episode 105 length 965 reward -21.0 epsilon 0.9045385428549331\n",
      "episode 106 length 1053 reward -20.0 epsilon 0.9030492999977557\n",
      "episode 107 length 824 reward -21.0 epsilon 0.9018839285691573\n",
      "episode 108 length 852 reward -21.0 epsilon 0.900678957140558\n",
      "episode 109 length 910 reward -21.0 epsilon 0.8993919571405282\n",
      "episode 110 length 1018 reward -19.0 epsilon 0.897952214283352\n",
      "episode 111 length 944 reward -21.0 epsilon 0.8966171285690354\n",
      "episode 112 length 854 reward -21.0 epsilon 0.8954093285690075\n",
      "episode 113 length 824 reward -21.0 epsilon 0.894243957140409\n",
      "episode 114 length 1333 reward -17.0 epsilon 0.8923587142832226\n",
      "episode 115 length 852 reward -21.0 epsilon 0.8911537428546232\n",
      "episode 116 length 1137 reward -19.0 epsilon 0.8895456999974432\n",
      "episode 117 length 1080 reward -20.0 epsilon 0.8880182714259792\n",
      "episode 118 length 842 reward -20.0 epsilon 0.8868274428545231\n",
      "episode 119 length 900 reward -21.0 epsilon 0.8855545857116365\n",
      "episode 120 length 940 reward -21.0 epsilon 0.8842251571401771\n",
      "episode 121 length 764 reward -21.0 epsilon 0.8831446428544378\n",
      "episode 122 length 856 reward -21.0 epsilon 0.8819340142829812\n",
      "episode 123 length 947 reward -21.0 epsilon 0.8805946857115217\n",
      "episode 124 length 950 reward -20.0 epsilon 0.8792511142829191\n",
      "episode 125 length 948 reward -20.0 epsilon 0.8779103714257452\n",
      "episode 126 length 978 reward -20.0 epsilon 0.8765271999971418\n",
      "episode 127 length 1008 reward -21.0 epsilon 0.8751015999971088\n",
      "episode 128 length 764 reward -21.0 epsilon 0.8740210857113695\n",
      "episode 129 length 904 reward -20.0 epsilon 0.8727425714256256\n",
      "episode 130 length 871 reward -20.0 epsilon 0.8715107285684542\n",
      "episode 131 length 813 reward -21.0 epsilon 0.8703609142827133\n",
      "episode 132 length 842 reward -20.0 epsilon 0.8691700857112572\n",
      "episode 133 length 886 reward -21.0 epsilon 0.867917028568371\n",
      "episode 134 length 1115 reward -19.0 epsilon 0.866340099996906\n",
      "episode 135 length 981 reward -19.0 epsilon 0.8649526857111596\n",
      "episode 136 length 825 reward -21.0 epsilon 0.8637858999968469\n",
      "episode 137 length 794 reward -21.0 epsilon 0.862662957139678\n",
      "episode 138 length 878 reward -20.0 epsilon 0.8614212142825064\n",
      "episode 139 length 1032 reward -21.0 epsilon 0.8599616714253298\n",
      "episode 140 length 792 reward -21.0 epsilon 0.8588415571395895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 141 length 889 reward -20.0 epsilon 0.8575842571395604\n",
      "episode 142 length 1087 reward -20.0 epsilon 0.8560469285680963\n",
      "episode 143 length 949 reward -20.0 epsilon 0.8547047714252081\n",
      "episode 144 length 1022 reward -20.0 epsilon 0.8532593714251746\n",
      "episode 145 length 824 reward -21.0 epsilon 0.8520939999965762\n",
      "episode 146 length 852 reward -21.0 epsilon 0.8508890285679769\n",
      "episode 147 length 991 reward -20.0 epsilon 0.8494874714250873\n",
      "episode 148 length 889 reward -20.0 epsilon 0.8482301714250582\n",
      "episode 149 length 1044 reward -20.0 epsilon 0.8467536571393097\n",
      "episode 150 length 1207 reward -19.0 epsilon 0.8450466142821274\n",
      "episode 151 length 879 reward -20.0 epsilon 0.8438034571392414\n",
      "episode 152 length 931 reward -20.0 epsilon 0.842486757139211\n",
      "episode 153 length 1039 reward -20.0 epsilon 0.8410173142820341\n",
      "episode 154 length 854 reward -21.0 epsilon 0.8398095142820061\n",
      "episode 155 length 856 reward -21.0 epsilon 0.8385988857105495\n",
      "episode 156 length 935 reward -19.0 epsilon 0.8372765285676618\n",
      "episode 157 length 880 reward -21.0 epsilon 0.8360319571390615\n",
      "episode 158 length 1001 reward -21.0 epsilon 0.8346162571390288\n",
      "episode 159 length 824 reward -21.0 epsilon 0.8334508857104304\n",
      "episode 160 length 922 reward -20.0 epsilon 0.8321469142818287\n",
      "episode 161 length 1085 reward -20.0 epsilon 0.8306124142817932\n",
      "episode 162 length 926 reward -20.0 epsilon 0.8293027857103343\n",
      "episode 163 length 1101 reward -19.0 epsilon 0.8277456571388697\n",
      "episode 164 length 866 reward -20.0 epsilon 0.8265208857102699\n",
      "episode 165 length 1083 reward -20.0 epsilon 0.824989214281663\n",
      "episode 166 length 1113 reward -19.0 epsilon 0.8234151142816266\n",
      "episode 167 length 947 reward -20.0 epsilon 0.822075785710167\n",
      "episode 168 length 912 reward -21.0 epsilon 0.8207859571387086\n",
      "episode 169 length 911 reward -20.0 epsilon 0.8194975428529645\n",
      "episode 170 length 1002 reward -21.0 epsilon 0.8180804285672174\n",
      "episode 171 length 870 reward -20.0 epsilon 0.8168499999957604\n",
      "episode 172 length 914 reward -21.0 epsilon 0.8155573428528733\n",
      "episode 173 length 783 reward -21.0 epsilon 0.8144499571385619\n",
      "episode 174 length 1127 reward -20.0 epsilon 0.812856057138525\n",
      "episode 175 length 852 reward -21.0 epsilon 0.8116510857099257\n",
      "episode 176 length 1012 reward -19.0 epsilon 0.8102198285670354\n",
      "episode 177 length 1066 reward -19.0 epsilon 0.808712199995572\n",
      "episode 178 length 933 reward -20.0 epsilon 0.8073926714241129\n",
      "episode 179 length 850 reward -21.0 epsilon 0.8061905285669422\n",
      "episode 180 length 824 reward -21.0 epsilon 0.8050251571383438\n",
      "episode 181 length 866 reward -20.0 epsilon 0.803800385709744\n",
      "episode 182 length 1086 reward -19.0 epsilon 0.8022644714239942\n",
      "episode 183 length 792 reward -21.0 epsilon 0.8011443571382539\n",
      "episode 184 length 824 reward -21.0 epsilon 0.7999789857096555\n",
      "episode 185 length 964 reward -20.0 epsilon 0.7986156142810525\n",
      "episode 186 length 854 reward -21.0 epsilon 0.7974078142810246\n",
      "episode 187 length 783 reward -21.0 epsilon 0.7963004285667132\n",
      "episode 188 length 824 reward -21.0 epsilon 0.7951350571381148\n",
      "episode 189 length 838 reward -20.0 epsilon 0.793949885709516\n",
      "episode 190 length 843 reward -20.0 epsilon 0.7927576428523455\n",
      "episode 191 length 838 reward -20.0 epsilon 0.7915724714237466\n",
      "episode 192 length 880 reward -21.0 epsilon 0.7903278999951464\n",
      "episode 193 length 870 reward -20.0 epsilon 0.7890974714236894\n",
      "episode 194 length 901 reward -21.0 epsilon 0.7878231999950884\n",
      "episode 195 length 792 reward -21.0 epsilon 0.7867030857093482\n",
      "episode 196 length 904 reward -20.0 epsilon 0.7854245714236043\n",
      "episode 197 length 946 reward -21.0 epsilon 0.7840866571378591\n",
      "episode 198 length 842 reward -20.0 epsilon 0.7828958285664029\n",
      "episode 199 length 919 reward -20.0 epsilon 0.7815960999949443\n",
      "episode 200 length 813 reward -21.0 epsilon 0.7804462857092034\n",
      "episode 201 length 889 reward -20.0 epsilon 0.7791889857091743\n",
      "episode 202 length 881 reward -21.0 epsilon 0.7779429999948597\n",
      "episode 203 length 980 reward -20.0 epsilon 0.7765569999948276\n",
      "episode 204 length 867 reward -21.0 epsilon 0.7753308142805135\n",
      "episode 205 length 938 reward -20.0 epsilon 0.7740042142804828\n",
      "episode 206 length 905 reward -21.0 epsilon 0.7727242857090246\n",
      "episode 207 length 930 reward -20.0 epsilon 0.7714089999947085\n",
      "episode 208 length 1006 reward -20.0 epsilon 0.7699862285661041\n",
      "episode 209 length 964 reward -21.0 epsilon 0.7686228571375011\n",
      "episode 210 length 838 reward -20.0 epsilon 0.7674376857089023\n",
      "episode 211 length 764 reward -21.0 epsilon 0.766357171423163\n",
      "episode 212 length 844 reward -21.0 epsilon 0.7651635142802782\n",
      "episode 213 length 842 reward -20.0 epsilon 0.763972685708822\n",
      "episode 214 length 965 reward -21.0 epsilon 0.7626078999945047\n",
      "episode 215 length 980 reward -20.0 epsilon 0.7612218999944727\n",
      "episode 216 length 912 reward -21.0 epsilon 0.7599320714230142\n",
      "episode 217 length 1113 reward -19.0 epsilon 0.7583579714229778\n",
      "episode 218 length 904 reward -20.0 epsilon 0.7570794571372339\n",
      "episode 219 length 905 reward -21.0 epsilon 0.7557995285657757\n",
      "episode 220 length 783 reward -21.0 epsilon 0.7546921428514644\n",
      "episode 221 length 843 reward -21.0 epsilon 0.7534998999942939\n",
      "episode 222 length 997 reward -19.0 epsilon 0.7520898571371184\n",
      "episode 223 length 824 reward -21.0 epsilon 0.75092448570852\n",
      "episode 224 length 1042 reward -20.0 epsilon 0.7494507999942002\n",
      "episode 225 length 792 reward -21.0 epsilon 0.74833068570846\n",
      "episode 226 length 842 reward -20.0 epsilon 0.7471398571370038\n",
      "episode 227 length 783 reward -21.0 epsilon 0.7460324714226925\n",
      "episode 228 length 899 reward -21.0 epsilon 0.7447610285655202\n",
      "episode 229 length 980 reward -20.0 epsilon 0.7433750285654881\n",
      "episode 230 length 843 reward -20.0 epsilon 0.7421827857083176\n",
      "episode 231 length 886 reward -21.0 epsilon 0.7409297285654315\n",
      "episode 232 length 792 reward -21.0 epsilon 0.7398096142796913\n",
      "episode 233 length 912 reward -21.0 epsilon 0.7385197857082328\n",
      "episode 234 length 764 reward -21.0 epsilon 0.7374392714224935\n",
      "episode 235 length 872 reward -20.0 epsilon 0.7362060142796079\n",
      "episode 236 length 764 reward -21.0 epsilon 0.7351254999938686\n",
      "episode 237 length 853 reward -21.0 epsilon 0.7339191142795549\n",
      "episode 238 length 843 reward -21.0 epsilon 0.7327268714223845\n",
      "episode 239 length 764 reward -21.0 epsilon 0.7316463571366452\n",
      "episode 240 length 824 reward -21.0 epsilon 0.7304809857080468\n",
      "episode 241 length 983 reward -20.0 epsilon 0.7290907428508717\n",
      "episode 242 length 878 reward -20.0 epsilon 0.7278489999937001\n",
      "episode 243 length 936 reward -20.0 epsilon 0.726525228565098\n",
      "episode 244 length 976 reward -21.0 epsilon 0.7251448857079232\n",
      "episode 245 length 845 reward -21.0 epsilon 0.7239498142793241\n",
      "episode 246 length 764 reward -21.0 epsilon 0.7228692999935848\n",
      "episode 247 length 922 reward -20.0 epsilon 0.7215653285649832\n",
      "episode 248 length 824 reward -21.0 epsilon 0.7203999571363848\n",
      "episode 249 length 854 reward -21.0 epsilon 0.7191921571363569\n",
      "episode 250 length 811 reward -21.0 epsilon 0.7180451714220446\n",
      "episode 251 length 886 reward -21.0 epsilon 0.7167921142791585\n",
      "episode 252 length 842 reward -20.0 epsilon 0.7156012857077023\n",
      "episode 253 length 980 reward -19.0 epsilon 0.7142152857076702\n",
      "episode 254 length 764 reward -21.0 epsilon 0.7131347714219309\n",
      "episode 255 length 955 reward -20.0 epsilon 0.7117841285647568\n",
      "episode 256 length 908 reward -21.0 epsilon 0.7104999571361557\n",
      "episode 257 length 890 reward -21.0 epsilon 0.7092412428504122\n",
      "episode 258 length 921 reward -20.0 epsilon 0.7079386857075249\n",
      "episode 259 length 878 reward -20.0 epsilon 0.7066969428503533\n",
      "episode 260 length 886 reward -21.0 epsilon 0.7054438857074672\n",
      "episode 261 length 838 reward -20.0 epsilon 0.7042587142788683\n",
      "episode 262 length 764 reward -21.0 epsilon 0.703178199993129\n",
      "episode 263 length 945 reward -19.0 epsilon 0.7018416999930981\n",
      "episode 264 length 838 reward -20.0 epsilon 0.7006565285644992\n",
      "episode 265 length 824 reward -21.0 epsilon 0.6994911571359008\n",
      "episode 266 length 862 reward -21.0 epsilon 0.6982720428501583\n",
      "episode 267 length 878 reward -20.0 epsilon 0.6970302999929867\n",
      "episode 268 length 841 reward -21.0 epsilon 0.6958408857072449\n",
      "episode 269 length 764 reward -21.0 epsilon 0.6947603714215056\n",
      "episode 270 length 842 reward -20.0 epsilon 0.6935695428500495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 271 length 764 reward -21.0 epsilon 0.6924890285643102\n",
      "episode 272 length 842 reward -20.0 epsilon 0.691298199992854\n",
      "episode 273 length 764 reward -21.0 epsilon 0.6902176857071147\n",
      "episode 274 length 902 reward -20.0 epsilon 0.6889419999927995\n",
      "episode 275 length 764 reward -21.0 epsilon 0.6878614857070602\n",
      "episode 276 length 871 reward -21.0 epsilon 0.6866296428498888\n",
      "episode 277 length 884 reward -21.0 epsilon 0.6853794142784313\n",
      "episode 278 length 764 reward -21.0 epsilon 0.684298899992692\n",
      "episode 279 length 794 reward -21.0 epsilon 0.6831759571355231\n",
      "episode 280 length 764 reward -21.0 epsilon 0.6820954428497839\n",
      "episode 281 length 856 reward -21.0 epsilon 0.6808848142783273\n",
      "episode 282 length 830 reward -21.0 epsilon 0.6797109571354429\n",
      "episode 283 length 933 reward -21.0 epsilon 0.6783914285639838\n",
      "episode 284 length 870 reward -20.0 epsilon 0.6771609999925268\n",
      "episode 285 length 764 reward -21.0 epsilon 0.6760804857067875\n",
      "episode 286 length 930 reward -20.0 epsilon 0.6747651999924713\n",
      "episode 287 length 792 reward -21.0 epsilon 0.6736450857067311\n",
      "episode 288 length 965 reward -21.0 epsilon 0.6722802999924138\n",
      "episode 289 length 764 reward -21.0 epsilon 0.6711997857066745\n",
      "episode 290 length 921 reward -20.0 epsilon 0.6698972285637872\n",
      "episode 291 length 921 reward -20.0 epsilon 0.6685946714208999\n",
      "episode 292 length 824 reward -21.0 epsilon 0.6674292999923015\n",
      "episode 293 length 1076 reward -19.0 epsilon 0.6659075285636948\n",
      "episode 294 length 792 reward -21.0 epsilon 0.6647874142779546\n",
      "episode 295 length 764 reward -21.0 epsilon 0.6637068999922153\n",
      "episode 296 length 884 reward -21.0 epsilon 0.6624566714207578\n",
      "episode 297 length 1007 reward -21.0 epsilon 0.6610324857064391\n",
      "episode 298 length 1065 reward -20.0 epsilon 0.65952627142069\n",
      "episode 299 length 764 reward -21.0 epsilon 0.6584457571349507\n",
      "episode 300 length 794 reward -21.0 epsilon 0.6573228142777818\n",
      "episode 301 length 889 reward -20.0 epsilon 0.6560655142777527\n",
      "episode 302 length 824 reward -21.0 epsilon 0.6549001428491543\n",
      "episode 303 length 843 reward -21.0 epsilon 0.6537078999919839\n",
      "episode 304 length 824 reward -21.0 epsilon 0.6525425285633855\n",
      "episode 305 length 965 reward -19.0 epsilon 0.6511777428490682\n",
      "episode 306 length 823 reward -21.0 epsilon 0.6500137857061841\n",
      "episode 307 length 878 reward -20.0 epsilon 0.6487720428490125\n",
      "episode 308 length 884 reward -21.0 epsilon 0.647521814277555\n",
      "episode 309 length 845 reward -21.0 epsilon 0.6463267428489559\n",
      "episode 310 length 884 reward -21.0 epsilon 0.6450765142774983\n",
      "episode 311 length 950 reward -20.0 epsilon 0.6437329428488958\n",
      "episode 312 length 942 reward -21.0 epsilon 0.6424006857060078\n",
      "episode 313 length 842 reward -20.0 epsilon 0.6412098571345517\n",
      "episode 314 length 854 reward -21.0 epsilon 0.6400020571345237\n",
      "episode 315 length 988 reward -20.0 epsilon 0.6386047428487771\n",
      "episode 316 length 792 reward -21.0 epsilon 0.6374846285630369\n",
      "episode 317 length 854 reward -21.0 epsilon 0.6362768285630089\n",
      "episode 318 length 820 reward -21.0 epsilon 0.6351171142772678\n",
      "episode 319 length 842 reward -20.0 epsilon 0.6339262857058117\n",
      "episode 320 length 976 reward -21.0 epsilon 0.6325459428486369\n",
      "episode 321 length 1067 reward -21.0 epsilon 0.6310368999914591\n",
      "episode 322 length 1088 reward -20.0 epsilon 0.6294981571342806\n",
      "episode 323 length 903 reward -21.0 epsilon 0.628221057134251\n",
      "episode 324 length 944 reward -21.0 epsilon 0.6268859714199344\n",
      "episode 325 length 854 reward -21.0 epsilon 0.6256781714199064\n",
      "episode 326 length 842 reward -20.0 epsilon 0.6244873428484503\n",
      "episode 327 length 997 reward -20.0 epsilon 0.6230772999912748\n",
      "episode 328 length 842 reward -20.0 epsilon 0.6218864714198187\n",
      "episode 329 length 852 reward -21.0 epsilon 0.6206814999912194\n",
      "episode 330 length 928 reward -20.0 epsilon 0.6193690428483318\n",
      "episode 331 length 764 reward -21.0 epsilon 0.6182885285625925\n",
      "episode 332 length 854 reward -21.0 epsilon 0.6170807285625646\n",
      "episode 333 length 1021 reward -21.0 epsilon 0.6156367428482454\n",
      "episode 334 length 764 reward -21.0 epsilon 0.6145562285625061\n",
      "episode 335 length 841 reward -21.0 epsilon 0.6133668142767643\n",
      "episode 336 length 884 reward -21.0 epsilon 0.6121165857053068\n",
      "episode 337 length 838 reward -20.0 epsilon 0.6109314142767079\n",
      "episode 338 length 935 reward -21.0 epsilon 0.6096090571338202\n",
      "episode 339 length 903 reward -21.0 epsilon 0.6083319571337906\n",
      "episode 340 length 882 reward -21.0 epsilon 0.6070845571337617\n",
      "episode 341 length 1071 reward -19.0 epsilon 0.6055698571337267\n",
      "episode 342 length 843 reward -20.0 epsilon 0.6043776142765562\n",
      "episode 343 length 843 reward -20.0 epsilon 0.6031853714193858\n",
      "episode 344 length 764 reward -21.0 epsilon 0.6021048571336465\n",
      "episode 345 length 921 reward -20.0 epsilon 0.6008022999907592\n",
      "episode 346 length 852 reward -21.0 epsilon 0.5995973285621599\n",
      "episode 347 length 794 reward -21.0 epsilon 0.598474385704991\n",
      "episode 348 length 916 reward -21.0 epsilon 0.5971788999906753\n",
      "episode 349 length 883 reward -21.0 epsilon 0.5959300857049321\n",
      "episode 350 length 866 reward -20.0 epsilon 0.5947053142763323\n",
      "episode 351 length 852 reward -21.0 epsilon 0.593500342847733\n",
      "episode 352 length 839 reward -21.0 epsilon 0.5923137571334198\n",
      "episode 353 length 928 reward -20.0 epsilon 0.5910012999905323\n",
      "episode 354 length 898 reward -20.0 epsilon 0.5897312714190743\n",
      "episode 355 length 843 reward -20.0 epsilon 0.5885390285619039\n",
      "episode 356 length 826 reward -21.0 epsilon 0.5873708285618768\n",
      "episode 357 length 824 reward -21.0 epsilon 0.5862054571332784\n",
      "episode 358 length 852 reward -21.0 epsilon 0.5850004857046791\n",
      "episode 359 length 884 reward -21.0 epsilon 0.5837502571332216\n",
      "episode 360 length 894 reward -20.0 epsilon 0.5824858857046209\n",
      "episode 361 length 854 reward -21.0 epsilon 0.5812780857045929\n",
      "episode 362 length 764 reward -21.0 epsilon 0.5801975714188536\n",
      "episode 363 length 838 reward -20.0 epsilon 0.5790123999902548\n",
      "episode 364 length 792 reward -21.0 epsilon 0.5778922857045146\n",
      "episode 365 length 843 reward -20.0 epsilon 0.5767000428473441\n",
      "episode 366 length 845 reward -21.0 epsilon 0.575504971418745\n",
      "episode 367 length 782 reward -21.0 epsilon 0.574398999990148\n",
      "episode 368 length 824 reward -21.0 epsilon 0.5732336285615496\n",
      "episode 369 length 879 reward -20.0 epsilon 0.5719904714186637\n",
      "episode 370 length 843 reward -20.0 epsilon 0.5707982285614932\n",
      "episode 371 length 824 reward -21.0 epsilon 0.5696328571328948\n",
      "episode 372 length 900 reward -20.0 epsilon 0.5683599999900082\n",
      "episode 373 length 931 reward -20.0 epsilon 0.5670432999899777\n",
      "episode 374 length 824 reward -21.0 epsilon 0.5658779285613793\n",
      "episode 375 length 824 reward -21.0 epsilon 0.5647125571327809\n",
      "episode 376 length 1085 reward -20.0 epsilon 0.5631780571327454\n",
      "episode 377 length 905 reward -20.0 epsilon 0.5618981285612872\n",
      "episode 378 length 852 reward -21.0 epsilon 0.5606931571326879\n",
      "episode 379 length 1067 reward -19.0 epsilon 0.5591841142755101\n",
      "episode 380 length 974 reward -20.0 epsilon 0.5578065999897639\n",
      "episode 381 length 841 reward -21.0 epsilon 0.5566171857040221\n",
      "episode 382 length 921 reward -20.0 epsilon 0.5553146285611348\n",
      "episode 383 length 866 reward -20.0 epsilon 0.554089857132535\n",
      "episode 384 length 871 reward -20.0 epsilon 0.5528580142753636\n",
      "episode 385 length 1118 reward -19.0 epsilon 0.5512768428467556\n",
      "episode 386 length 965 reward -20.0 epsilon 0.5499120571324383\n",
      "episode 387 length 944 reward -21.0 epsilon 0.5485769714181217\n",
      "episode 388 length 920 reward -20.0 epsilon 0.5472758285609487\n",
      "episode 389 length 842 reward -20.0 epsilon 0.5460849999894926\n",
      "episode 390 length 844 reward -20.0 epsilon 0.5448913428466078\n",
      "episode 391 length 792 reward -21.0 epsilon 0.5437712285608676\n",
      "episode 392 length 838 reward -20.0 epsilon 0.5425860571322687\n",
      "episode 393 length 886 reward -21.0 epsilon 0.5413329999893826\n",
      "episode 394 length 898 reward -20.0 epsilon 0.5400629714179246\n",
      "episode 395 length 891 reward -20.0 epsilon 0.5388028428464668\n",
      "episode 396 length 859 reward -20.0 epsilon 0.5375879714178673\n",
      "episode 397 length 852 reward -21.0 epsilon 0.536382999989268\n",
      "episode 398 length 884 reward -21.0 epsilon 0.5351327714178105\n",
      "episode 399 length 894 reward -20.0 epsilon 0.5338683999892098\n",
      "episode 400 length 1048 reward -20.0 epsilon 0.532386228560604\n",
      "episode 401 length 886 reward -21.0 epsilon 0.5311331714177179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 402 length 919 reward -20.0 epsilon 0.5298334428462592\n",
      "episode 403 length 783 reward -21.0 epsilon 0.5287260571319479\n",
      "episode 404 length 843 reward -21.0 epsilon 0.5275338142747774\n",
      "episode 405 length 912 reward -21.0 epsilon 0.526243985703319\n",
      "episode 406 length 926 reward -20.0 epsilon 0.5249343571318601\n",
      "episode 407 length 824 reward -21.0 epsilon 0.5237689857032617\n",
      "episode 408 length 792 reward -21.0 epsilon 0.5226488714175215\n",
      "episode 409 length 926 reward -20.0 epsilon 0.5213392428460626\n",
      "episode 410 length 782 reward -21.0 epsilon 0.5202332714174656\n",
      "episode 411 length 1024 reward -20.0 epsilon 0.5187850428460035\n",
      "episode 412 length 948 reward -21.0 epsilon 0.5174442999888296\n",
      "episode 413 length 820 reward -21.0 epsilon 0.5162845857030884\n",
      "episode 414 length 895 reward -20.0 epsilon 0.5150187999887734\n",
      "episode 415 length 842 reward -20.0 epsilon 0.5138279714173173\n",
      "episode 416 length 822 reward -21.0 epsilon 0.5126654285601475\n",
      "episode 417 length 898 reward -20.0 epsilon 0.5113953999886895\n",
      "episode 418 length 824 reward -21.0 epsilon 0.5102300285600911\n",
      "episode 419 length 926 reward -20.0 epsilon 0.5089203999886323\n",
      "episode 420 length 954 reward -20.0 epsilon 0.5075711714171725\n",
      "episode 421 length 820 reward -21.0 epsilon 0.5064114571314313\n",
      "episode 422 length 824 reward -21.0 epsilon 0.5052460857028329\n",
      "episode 423 length 838 reward -20.0 epsilon 0.504060914274234\n",
      "episode 424 length 832 reward -21.0 epsilon 0.5028842285599211\n",
      "episode 425 length 1097 reward -21.0 epsilon 0.5013327571313138\n",
      "episode 426 length 854 reward -21.0 epsilon 0.5001249571312858\n",
      "episode 427 length 843 reward -20.0 epsilon 0.49893271427415725\n",
      "episode 428 length 916 reward -21.0 epsilon 0.4976372285598924\n",
      "episode 429 length 783 reward -21.0 epsilon 0.4965298428456245\n",
      "episode 430 length 917 reward -20.0 epsilon 0.4952329428456454\n",
      "episode 431 length 782 reward -21.0 epsilon 0.4941269714170918\n",
      "episode 432 length 888 reward -21.0 epsilon 0.4928710857028263\n",
      "episode 433 length 826 reward -21.0 epsilon 0.4917028857028451\n",
      "episode 434 length 794 reward -21.0 epsilon 0.4905799428457203\n",
      "episode 435 length 845 reward -21.0 epsilon 0.48938487141716813\n",
      "episode 436 length 826 reward -21.0 epsilon 0.48821667141718694\n",
      "episode 437 length 932 reward -20.0 epsilon 0.4868985571314939\n",
      "episode 438 length 783 reward -21.0 epsilon 0.485791171417226\n",
      "episode 439 length 824 reward -21.0 epsilon 0.48462579998867333\n",
      "episode 440 length 825 reward -21.0 epsilon 0.4834590142744064\n",
      "episode 441 length 794 reward -21.0 epsilon 0.48233607141728163\n",
      "episode 442 length 826 reward -21.0 epsilon 0.48116787141730044\n",
      "episode 443 length 782 reward -21.0 epsilon 0.4800618999887468\n",
      "episode 444 length 890 reward -21.0 epsilon 0.4788031857030528\n",
      "episode 445 length 886 reward -21.0 epsilon 0.47755012856021584\n",
      "episode 446 length 1029 reward -19.0 epsilon 0.47609482856023927\n",
      "episode 447 length 886 reward -21.0 epsilon 0.4748417714174023\n",
      "episode 448 length 904 reward -20.0 epsilon 0.4735632571317086\n",
      "episode 449 length 783 reward -21.0 epsilon 0.4724558714174407\n",
      "episode 450 length 902 reward -20.0 epsilon 0.47118018570317555\n",
      "episode 451 length 839 reward -21.0 epsilon 0.46999359998890894\n",
      "episode 452 length 826 reward -21.0 epsilon 0.46882539998892775\n",
      "episode 453 length 854 reward -21.0 epsilon 0.4676175999889472\n",
      "episode 454 length 824 reward -21.0 epsilon 0.46645222856039453\n",
      "episode 455 length 824 reward -21.0 epsilon 0.46528685713184187\n",
      "episode 456 length 853 reward -21.0 epsilon 0.4640804714175756\n",
      "episode 457 length 1044 reward -19.0 epsilon 0.46260395713188507\n",
      "episode 458 length 794 reward -21.0 epsilon 0.4614810142747603\n",
      "episode 459 length 990 reward -20.0 epsilon 0.46008087141764\n",
      "episode 460 length 856 reward -21.0 epsilon 0.4588702428462309\n",
      "episode 461 length 824 reward -21.0 epsilon 0.45770487141767824\n",
      "episode 462 length 792 reward -21.0 epsilon 0.456584757131982\n",
      "episode 463 length 1235 reward -19.0 epsilon 0.45483811427486726\n",
      "episode 464 length 764 reward -21.0 epsilon 0.45375759998917037\n",
      "episode 465 length 782 reward -21.0 epsilon 0.45265162856061675\n",
      "episode 466 length 1037 reward -20.0 epsilon 0.4511850142749261\n",
      "episode 467 length 1279 reward -17.0 epsilon 0.4493761428463838\n",
      "episode 468 length 903 reward -20.0 epsilon 0.44809904284640434\n",
      "episode 469 length 783 reward -21.0 epsilon 0.44699165713213646\n",
      "episode 470 length 881 reward -21.0 epsilon 0.4457456714178708\n",
      "episode 471 length 783 reward -21.0 epsilon 0.4446382857036029\n",
      "episode 472 length 824 reward -21.0 epsilon 0.44347291427505026\n",
      "episode 473 length 824 reward -21.0 epsilon 0.4423075428464976\n",
      "episode 474 length 871 reward -20.0 epsilon 0.44107569998937457\n",
      "episode 475 length 826 reward -21.0 epsilon 0.4399074999893934\n",
      "episode 476 length 916 reward -21.0 epsilon 0.4386120142751285\n",
      "episode 477 length 824 reward -21.0 epsilon 0.43744664284657586\n",
      "episode 478 length 1057 reward -21.0 epsilon 0.43595174284659993\n",
      "episode 479 length 844 reward -21.0 epsilon 0.434758085703762\n",
      "episode 480 length 853 reward -21.0 epsilon 0.4335516999894957\n",
      "episode 481 length 842 reward -20.0 epsilon 0.4323608714180863\n",
      "episode 482 length 862 reward -21.0 epsilon 0.43114175713239167\n",
      "episode 483 length 838 reward -20.0 epsilon 0.4299565857038393\n",
      "episode 484 length 960 reward -20.0 epsilon 0.4285988714181469\n",
      "episode 485 length 824 reward -21.0 epsilon 0.42743349998959423\n",
      "episode 486 length 854 reward -21.0 epsilon 0.4262256999896137\n",
      "episode 487 length 826 reward -21.0 epsilon 0.4250574999896325\n",
      "episode 488 length 826 reward -21.0 epsilon 0.4238892999896513\n",
      "episode 489 length 824 reward -21.0 epsilon 0.42272392856109864\n",
      "episode 490 length 960 reward -20.0 epsilon 0.4213662142754062\n",
      "episode 491 length 783 reward -21.0 epsilon 0.42025882856113833\n",
      "episode 492 length 812 reward -21.0 epsilon 0.4191104285611568\n",
      "episode 493 length 842 reward -20.0 epsilon 0.4179195999897474\n",
      "episode 494 length 792 reward -21.0 epsilon 0.4167994857040512\n",
      "episode 495 length 886 reward -21.0 epsilon 0.4155464285612142\n",
      "episode 496 length 824 reward -21.0 epsilon 0.41438105713266155\n",
      "episode 497 length 944 reward -21.0 epsilon 0.41304597141839733\n",
      "episode 498 length 884 reward -21.0 epsilon 0.4117957428469889\n",
      "episode 499 length 843 reward -20.0 epsilon 0.41060349998986523\n",
      "episode 500 length 945 reward -19.0 epsilon 0.40926699998988675\n",
      "episode 501 length 824 reward -21.0 epsilon 0.4081016285613341\n",
      "episode 502 length 824 reward -21.0 epsilon 0.4069362571327814\n",
      "episode 503 length 886 reward -21.0 epsilon 0.40568319998994445\n",
      "episode 504 length 824 reward -21.0 epsilon 0.4045178285613918\n",
      "episode 505 length 782 reward -21.0 epsilon 0.40341185713283817\n",
      "episode 506 length 843 reward -20.0 epsilon 0.4022196142757145\n",
      "episode 507 length 794 reward -21.0 epsilon 0.40109667141858973\n",
      "episode 508 length 824 reward -21.0 epsilon 0.39993129999003707\n",
      "episode 509 length 902 reward -20.0 epsilon 0.3986556142757719\n",
      "episode 510 length 826 reward -21.0 epsilon 0.3974874142757907\n",
      "episode 511 length 922 reward -20.0 epsilon 0.3961834428472403\n",
      "episode 512 length 824 reward -21.0 epsilon 0.3950180714186876\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-86869a1ff5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-edcbb9ea4453>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, env)\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0;31m# Q-Target = r + γQ(s’,argmax(Q(s’,a,ϴ),ϴ’))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;31m#print(targets[i].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model(model, Wrapped_Game(env))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
