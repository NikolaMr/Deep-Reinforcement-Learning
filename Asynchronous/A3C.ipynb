{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 80\n",
    "IMG_HEIGHT = 80\n",
    "CNT_CHANNELS = 1\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 0.0025\n",
    "POLICY_MODIFIER = 4\n",
    "ENTROPY_MODIFIER = 0.04\n",
    "MAX_STEPS = 50\n",
    "DISCOUNT = 0.99\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'\n",
    "#MAX_ITERATIONS = 100000000\n",
    "MAX_EP_LENGTH = 500\n",
    "#MAX_LEARNING_TIME = 7 * 60 * 60 # 7 hours\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s1, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_CHANNELS], dtype=tf.float32)\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 32, 8, stride=4, activation_fn=tf.nn.relu, padding='VALID')\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, stride=2, activation_fn=tf.nn.relu, padding='VALID')\n",
    "            conv3 = tf.contrib.layers.conv2d(conv1, 64, 3, stride=1, activation_fn=tf.nn.relu, padding='VALID')\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 256, activation_fn=tf.nn.relu)\n",
    "            \n",
    "            normalization = tf.layers.batch_normalization(embedding)\n",
    "            \n",
    "            step_size = tf.shape(self.X)[:1]\n",
    "            \n",
    "            rnn_in = tf.expand_dims(normalization, axis=0)\n",
    "            \n",
    "            self.lstm = tf.contrib.rnn.BasicLSTMCell(192)\n",
    "            self.state_c, self.state_h = self.lstm.zero_state(1, tf.float32)\n",
    "            \n",
    "            self.initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(tf.Variable(self.state_c, trainable=False), \\\n",
    "                                          tf.Variable(self.state_h, trainable=False))\n",
    "            \n",
    "            output, final_state = tf.nn.dynamic_rnn(self.lstm, rnn_in,sequence_length=step_size, initial_state=self.initial_lstm_state, dtype=tf.float32)\n",
    "            self.final_lstm_state = final_state\n",
    "            output = tf.reshape(output, (-1, 192))\n",
    "            \n",
    "            output = tf.layers.batch_normalization(output)\n",
    "            \n",
    "            self.policy = tf.contrib.layers.fully_connected(output, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=normalized_columns_initializer(0.01), biases_initializer=None)\n",
    "            self.value = tf.contrib.layers.fully_connected(output, 1, activation_fn=None, weights_initializer=normalized_columns_initializer(1.), biases_initializer=None)\n",
    "            \n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32)\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "\n",
    "                self.log_likelihood = tf.log(tf.reduce_sum(self.policy * self.actions_oh, axis=1) + 1e-8)\n",
    "                self.value_loss = tf.reduce_sum(tf.squared_difference(self.value, self.target_values))\n",
    "                self.policy_loss = -tf.reduce_sum(self.log_likelihood * self.advantages)\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_loss = -tf.reduce_sum(self.policy * -tf.log(self.policy + 1e-8))\n",
    "                self.loss = VALUE_MODIFIER * self.value_loss + \\\n",
    "                            POLICY_MODIFIER * self.policy_loss + \\\n",
    "                            ENTROPY_MODIFIER * self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                grads, grad_norms = tf.clip_by_global_norm(grads, 40.)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(grad, var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state, initial_lstm_state):\n",
    "        policy, final_lstm_state = sess.run((self.policy, self.final_lstm_state), \\\n",
    "                                            feed_dict={\\\n",
    "                                                       self.initial_lstm_state:initial_lstm_state, self.X:state\\\n",
    "                                                      }\\\n",
    "                                           )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction, final_lstm_state\n",
    "            \n",
    "    def act(self, sess, state, initial_lstm_state):\n",
    "        prediction, final_lstm_state = self.predict(sess, state, initial_lstm_state)\n",
    "        a = prediction\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state, final_lstm_state\n",
    "    \n",
    "    def get_value(self, sess, state, initial_lstm_state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.initial_lstm_state:initial_lstm_state, self.X:state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages, initial_lstm_state):\n",
    "        gu, value_loss, policy_loss, entropy_loss, final_lstm_state = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss, self.final_lstm_state), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages,\n",
    "                         self.initial_lstm_state: initial_lstm_state\n",
    "                     })\n",
    "        return value_loss / len(states), policy_loss / len(states), entropy_loss / len(states), final_lstm_state     \n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        global start_time\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        last_final_lstm_state = None\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "                self.agent.update_to_global(sess)\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                advantages = []\n",
    "                target_values = []\n",
    "                \n",
    "                initial_lstm_state = last_final_lstm_state\n",
    "                \n",
    "                if last_final_lstm_state == None:\n",
    "                        state_c = np.zeros((1, self.agent.lstm.state_size.c), np.float32)\n",
    "                        state_h = np.zeros((1, self.agent.lstm.state_size.h), np.float32)\n",
    "                        initial_lstm_state = [state_c, state_h]\n",
    "                \n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    val = self.agent.get_value(sess, s, initial_lstm_state)\n",
    "                    s, a, r, d, ns, initial_lstm_state = self.agent.act(sess, s, initial_lstm_state)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                \n",
    "                R = 0\n",
    "                if not done:\n",
    "                    R = last_values[-1]#self.agent.get_value(sess, s, initial_lstm_state)\n",
    "                \n",
    "                advantages = [0 for i in range(len(values))]\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    idx = len(rewards) - 1 - i\n",
    "                    reward = rewards[idx]\n",
    "                    R += DISCOUNT * reward\n",
    "                    advantage = (R - values[idx])\n",
    "                    advantages[idx] = advantage\n",
    "                    last_advantages.append(advantage)\n",
    "                    \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = last_values[-1]\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                states = np.vstack(states)\n",
    "                actions = np.vstack(actions).ravel()\n",
    "                target_values = np.vstack(target_values).ravel()\n",
    "                advantages = np.vstack(advantages).ravel()\n",
    "                \n",
    "                if last_final_lstm_state == None:\n",
    "                    state_c = np.zeros((1, self.agent.lstm.state_size.c), np.float32)\n",
    "                    state_h = np.zeros((1, self.agent.lstm.state_size.h), np.float32)\n",
    "                    last_final_lstm_state = [state_c, state_h]\n",
    "                \n",
    "                value_loss, policy_loss, entropy_loss, last_final_lstm_state = \\\n",
    "                    self.agent.train(sess, states, actions, target_values, advantages, last_final_lstm_state)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                \n",
    "                if done:\n",
    "                    last_final_lstm_state = None\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "#global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.GradientDescentOptimizer(LEARNING_RATE))\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    global global_counter\n",
    "    \n",
    "    MAX_MODELS = 1\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #save model every 15 minutes\n",
    "        while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            print(\"Current global iteration\", global_counter)\n",
    "            cnt_model += 1\n",
    "            time.sleep(15 * 60)\n",
    "        print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "building agent: local0\n",
      "building model\n",
      "building agent: local1\n",
      "building model\n",
      "building agent: local2\n",
      "building model\n",
      "building agent: local3\n",
      "building model\n",
      "building agent: local4\n",
      "building model\n",
      "building agent: local5\n",
      "building model\n",
      "building agent: local6\n",
      "building model\n",
      "building agent: local7\n",
      "building model\n",
      "building agent: local8\n",
      "building model\n",
      "building agent: local9\n",
      "building model\n",
      "building agent: local10\n",
      "building model\n",
      "building agent: local11\n",
      "building model\n",
      "building agent: local12\n",
      "building model\n",
      "building agent: local13\n",
      "building model\n",
      "building agent: local14\n",
      "building model\n",
      "building agent: local15\n",
      "building model\n",
      "building agent: local16\n",
      "building model\n",
      "building agent: local17\n",
      "building model\n",
      "building agent: local18\n",
      "building model\n",
      "building agent: local19\n",
      "worker starting agent: local0\n",
      "worker starting agent: local1\n",
      "worker starting agent: local2\n",
      "worker starting agent: local3\n",
      "worker starting agent: local4\n",
      "worker starting agent: local5\n",
      "worker starting agent: local6\n",
      "worker starting agent: local7\n",
      "worker starting agent: local8\n",
      "worker starting agent: local9\n",
      "worker starting agent: local10\n",
      "worker starting agent: local11\n",
      "worker starting agent: local12\n",
      "worker starting agent: local13\n",
      "worker starting agent: local14\n",
      "worker starting agent: local15\n",
      "worker starting agent: local16\n",
      "worker starting agent: local17\n",
      "worker starting agent: local18\n",
      "worker starting agent: local19\n",
      "Current model save name: model_0\n",
      "Current global iteration 141\n",
      "Current model save name: model_0\n",
      "Current global iteration 139925\n",
      "Current model save name: model_0\n",
      "Current global iteration 277520\n",
      "Current model save name: model_0\n",
      "Current global iteration 417477\n",
      "Current model save name: model_0\n",
      "Current global iteration 560158\n",
      "Current model save name: model_0\n",
      "Current global iteration 702598\n",
      "Current model save name: model_0\n",
      "Current global iteration 845276\n",
      "Current model save name: model_0\n",
      "Current global iteration 987803\n",
      "Current model save name: model_0\n",
      "Current global iteration 1130104\n",
      "Current model save name: model_0\n",
      "Current global iteration 1272866\n",
      "Current model save name: model_0\n",
      "Current global iteration 1415486\n",
      "Current model save name: model_0\n",
      "Current global iteration 1557751\n",
      "Current model save name: model_0\n",
      "Current global iteration 1700162\n",
      "Current model save name: model_0\n",
      "Current global iteration 1842383\n",
      "Current model save name: model_0\n",
      "Current global iteration 1985009\n",
      "Current model save name: model_0\n",
      "Current global iteration 2127397\n",
      "Current model save name: model_0\n",
      "Current global iteration 2269996\n",
      "Current model save name: model_0\n",
      "Current global iteration 2412702\n",
      "Current model save name: model_0\n",
      "Current global iteration 2555126\n",
      "Current model save name: model_0\n",
      "Current global iteration 2697542\n",
      "Current model save name: model_0\n",
      "Current global iteration 2839864\n",
      "Current model save name: model_0\n",
      "Current global iteration 2982351\n",
      "Current model save name: model_0\n",
      "Current global iteration 3124864\n",
      "Current model save name: model_0\n",
      "Current global iteration 3267515\n",
      "Current model save name: model_0\n",
      "Current global iteration 3409908\n",
      "Current model save name: model_0\n",
      "Current global iteration 3552869\n",
      "Current model save name: model_0\n",
      "Current global iteration 3695499\n",
      "Current model save name: model_0\n",
      "Current global iteration 3837716\n",
      "Current model save name: model_0\n",
      "Current global iteration 3980273\n",
      "Current model save name: model_0\n",
      "Current global iteration 4122792\n",
      "Current model save name: model_0\n",
      "Current global iteration 4265391\n",
      "Current model save name: model_0\n",
      "Current global iteration 4407868\n",
      "Current model save name: model_0\n",
      "Current global iteration 4550499\n",
      "Current model save name: model_0\n",
      "Current global iteration 4692901\n",
      "Current model save name: model_0\n",
      "Current global iteration 4835320\n",
      "Current model save name: model_0\n",
      "Current global iteration 4977533\n",
      "Current model save name: model_0\n",
      "Current global iteration 5120005\n",
      "Current model save name: model_0\n",
      "Current global iteration 5262222\n",
      "Current model save name: model_0\n",
      "Current global iteration 5404794\n",
      "Current model save name: model_0\n",
      "Current global iteration 5547115\n",
      "Current model save name: model_0\n",
      "Current global iteration 5689712\n",
      "Current model save name: model_0\n",
      "Current global iteration 5832459\n",
      "Current model save name: model_0\n",
      "Current global iteration 5971699\n",
      "Current model save name: model_0\n",
      "Current global iteration 6109145\n",
      "Current model save name: model_0\n",
      "Current global iteration 6246419\n",
      "Current model save name: model_0\n",
      "Current global iteration 6386727\n",
      "Current model save name: model_0\n",
      "Current global iteration 6528919\n",
      "Current model save name: model_0\n",
      "Current global iteration 6671219\n",
      "Current model save name: model_0\n",
      "Current global iteration 6813750\n",
      "Current model save name: model_0\n",
      "Current global iteration 6955831\n",
      "Current model save name: model_0\n",
      "Current global iteration 7098220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c46cb9c4a96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworker_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnt_threads = 20\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.25)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_fun(test_agent):\n",
    "    test_env = EnvWrapper(ENV_NAME)\n",
    "    #test_agent = Agent(test_env, 'tester', optimizer)\n",
    "    test_agent.update_to_global(sess)\n",
    "\n",
    "    state_c = np.zeros((1, test_agent.lstm.state_size.c), np.float32)\n",
    "    state_h = np.zeros((1, test_agent.lstm.state_size.h), np.float32)\n",
    "    initial_lstm_state = [state_c, state_h]\n",
    "\n",
    "    done = False\n",
    "    state = test_env.reset()\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        policy, final_lstm_state = sess.run((test_agent.policy, test_agent.final_lstm_state), \\\n",
    "                                                feed_dict={\\\n",
    "                                                           test_agent.initial_lstm_state:initial_lstm_state, test_agent.X:state\\\n",
    "                                                          }\\\n",
    "                                               )\n",
    "        initial_lstm_state = final_lstm_state\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        prediction = np.random.choice(test_env.action_space.n, p=policy)\n",
    "        if random.random() < 0.05:\n",
    "            prediction = env.action_space.sample()\n",
    "\n",
    "        ns, r, d, _ = test_env.step(prediction)\n",
    "        test_env.env.render()\n",
    "        state = ns\n",
    "        reward += (r)\n",
    "        done = d\n",
    "    test_env.env.close()\n",
    "    print('final reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = EnvWrapper(ENV_NAME)\n",
    "tester_agent = global_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_fun(tester_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
