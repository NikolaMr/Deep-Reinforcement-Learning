{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "CNT_CHANNELS = 1\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 0.5\n",
    "POLICY_MODIFIER = 1.0\n",
    "ENTROPY_MODIFIER = 0.075\n",
    "MAX_STEPS = 30\n",
    "DISCOUNT = 0.99\n",
    "#ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "ENV_NAME = 'PongDeterministic-v4'\n",
    "MAX_ITERATIONS = 1000000\n",
    "MAX_EP_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s1, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_CHANNELS], dtype=tf.float32)\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 16, 8, stride=4, padding='VALID')\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 32, 8, stride=2, padding='VALID')\n",
    "            flattened = tf.contrib.layers.flatten(conv2)\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 256, activation_fn=tf.nn.elu)\n",
    "            \n",
    "            step_size = tf.shape(self.X)[:1]\n",
    "            \n",
    "            rnn_in = tf.expand_dims(embedding, axis=0)\n",
    "            \n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(256)\n",
    "            output, state = tf.nn.dynamic_rnn(lstm, rnn_in,sequence_length=step_size, dtype=tf.float32)\n",
    "            output = tf.reshape(output, (-1, 256))\n",
    "            \n",
    "            self.policy = tf.contrib.layers.fully_connected(output, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=tf.random_normal_initializer(0.0, 0.01))\n",
    "            self.value = tf.contrib.layers.fully_connected(output, 1, activation_fn=None, weights_initializer=tf.random_normal_initializer(0.0, 0.5))\n",
    "            \n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32)\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "\n",
    "                self.log_likelihood = tf.log(tf.reduce_sum(self.policy * self.actions_oh, axis=1))\n",
    "                self.value_loss = tf.reduce_mean(tf.squared_difference(self.value, self.target_values))\n",
    "                self.policy_loss = -tf.reduce_mean(self.log_likelihood * self.advantages)\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_loss = -tf.reduce_mean(self.policy * -tf.log(self.policy))\n",
    "                self.loss = VALUE_MODIFIER * self.value_loss + \\\n",
    "                            POLICY_MODIFIER * self.policy_loss + \\\n",
    "                            ENTROPY_MODIFIER * self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -40., 40.), var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        policy = sess.run(self.policy, feed_dict={self.X:state}).flatten()\n",
    "        #print('cur policy', policy)\n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction\n",
    "            \n",
    "    def act(self, sess, state):\n",
    "        ret = self.predict(sess, state)\n",
    "        a = ret\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state\n",
    "    \n",
    "    def get_value(self, sess, state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.X: state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages):\n",
    "        gu, value_loss, policy_loss, entropy_loss = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages\n",
    "                     })\n",
    "        return value_loss, policy_loss, entropy_loss      \n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_counter = 0\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while global_counter <= MAX_ITERATIONS:\n",
    "                self.agent.update_to_global(sess)\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                advantages = []\n",
    "                target_values = []\n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    s, a, r, d, ns = self.agent.act(sess, s)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    val = self.agent.get_value(sess, s)\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    if global_counter % 10000 == 0 and self.agent.scope_name == \"local0\":\n",
    "                        print('agent local0 at iteration', global_counter)\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                R = 0\n",
    "                if not done:\n",
    "                    R = self.agent.get_value(sess, s)\n",
    "                \n",
    "                advantages = [0 for i in range(len(values))]\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    idx = len(rewards) - 1 - i\n",
    "                    reward = rewards[idx]\n",
    "                    R += DISCOUNT * reward\n",
    "                    advantage = (R - values[idx])\n",
    "                    advantages[idx] = advantage\n",
    "                    last_advantages.append(advantage)\n",
    "                    \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = self.agent.get_value(sess, s)\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                states = np.vstack(states)\n",
    "                actions = np.vstack(actions).ravel()\n",
    "                target_values = np.vstack(target_values).ravel()\n",
    "                advantages = np.vstack(advantages).ravel()\n",
    "                value_loss, policy_loss, entropy_loss = self.agent.train(sess, states, actions, target_values, advantages)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "sess = tf.Session()\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    MAX_MODELS = 5\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        #save model every 15 minutes\n",
    "        while global_counter <= MAX_ITERATIONS:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            time.sleep(15 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "building agent: local0\n",
      "building model\n",
      "building agent: local1\n",
      "building model\n",
      "building agent: local2\n",
      "building model\n",
      "building agent: local3\n",
      "building model\n",
      "building agent: local4\n",
      "building model\n",
      "building agent: local5\n",
      "building model\n",
      "building agent: local6\n",
      "building model\n",
      "building agent: local7\n",
      "building model\n",
      "building agent: local8\n",
      "building model\n",
      "building agent: local9\n",
      "building model\n",
      "building agent: local10\n",
      "building model\n",
      "building agent: local11\n",
      "building model\n",
      "building agent: local12\n",
      "building model\n",
      "building agent: local13\n",
      "building model\n",
      "building agent: local14\n",
      "building model\n",
      "building agent: local15\n",
      "building model\n",
      "building agent: local16\n",
      "building model\n",
      "building agent: local17\n",
      "building model\n",
      "building agent: local18\n",
      "building model\n",
      "building agent: local19\n",
      "worker starting agent: local0\n",
      "worker starting agent: local1\n",
      "worker starting agent: local2\n",
      "worker starting agent: local3\n",
      "worker starting agent: local4\n",
      "worker starting agent: local5\n",
      "worker starting agent: local6\n",
      "worker starting agent: local7\n",
      "worker starting agent: local8\n",
      "worker starting agent: local9\n",
      "worker starting agent: local10\n",
      "worker starting agent: local11\n",
      "worker starting agent: local12\n",
      "worker starting agent: local13\n",
      "worker starting agent: local14\n",
      "worker starting agent: local15\n",
      "worker starting agent: local16\n",
      "worker starting agent: local17\n",
      "worker starting agent: local18\n",
      "worker starting agent: local19\n",
      "Current model save name: model_0\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "cnt_threads = 20\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
