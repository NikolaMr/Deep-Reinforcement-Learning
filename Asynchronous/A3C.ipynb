{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 105\n",
    "IMG_HEIGHT = 80\n",
    "CNT_FRAMES = 1\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 0.5\n",
    "POLICY_MODIFIER = 1\n",
    "ENTROPY_MODIFIER = 5*1e-2\n",
    "MAX_STEPS = 30\n",
    "DISCOUNT = 0.99\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'\n",
    "#MAX_ITERATIONS = 100000000\n",
    "MAX_EP_LENGTH = 1000\n",
    "#MAX_LEARNING_TIME = 7 * 60 * 60 # 7 hours\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIP_VALUE = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        return s1, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            weights_initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_FRAMES], dtype=tf.float32, name='input')\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 32, 8, stride=4, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='first_conv')\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, stride=2, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='second_conv')\n",
    "            conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, stride=1, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                             weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='third_conv')\n",
    "            flattened = tf.contrib.layers.flatten(conv3, scope='flatten')\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=tf.random_normal_initializer(stddev=0.02), biases_initializer=bias_initializer,\\\n",
    "                                                         scope='fc_embed')\n",
    "\n",
    "            step_size = tf.shape(self.X)[:1]\n",
    "\n",
    "            rnn_in = tf.expand_dims(embedding, axis=0)\n",
    "\n",
    "            LSTM_CELL_CNT = 256\n",
    "\n",
    "            self.lstm = tf.contrib.rnn.BasicLSTMCell(LSTM_CELL_CNT)\n",
    "            self.c_in = tf.placeholder(tf.float32, [1, self.lstm.state_size.c],\n",
    "            \"c_in\")\n",
    "            self.h_in = tf.placeholder(tf.float32, [1, self.lstm.state_size.h],\n",
    "            \"h_in\")\n",
    "\n",
    "            self.initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(self.c_in, \\\n",
    "                                          self.h_in)\n",
    "\n",
    "            output, final_state = tf.nn.dynamic_rnn(self.lstm, rnn_in,sequence_length=step_size, initial_state=self.initial_lstm_state, dtype=tf.float32)\n",
    "            self.final_lstm_state = final_state\n",
    "            output = tf.reshape(output, (-1, LSTM_CELL_CNT))\n",
    "\n",
    "            self.policy = tf.contrib.layers.fully_connected(output, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=tf.random_normal_initializer(stddev=0.5), biases_initializer=None,\\\n",
    "                                                           scope='fc_policy')\n",
    "            self.value = tf.contrib.layers.fully_connected(\\\n",
    "                                                           output, \\\n",
    "                                                           1, \\\n",
    "                                                           activation_fn=None, \\\n",
    "                                                           weights_initializer=tf.random_normal_initializer(stddev=.25), \\\n",
    "                                                           biases_initializer=None,\\\n",
    "                                                          scope='fc_value')\n",
    "\n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32, name='actions')\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32, name='actions_oh')\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32, name='target_vals')\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32, name='advantages')\n",
    "\n",
    "                MIN_POLICY = 1e-8\n",
    "                MAX_POLICY = 1.0 - MIN_POLICY\n",
    "\n",
    "                self.log_policy = tf.log(tf.clip_by_value(self.policy, MIN_POLICY, MAX_POLICY), name='log_policy')\n",
    "\n",
    "                self.log_policy_for_action = tf.reduce_sum(tf.multiply(self.log_policy, self.actions_oh), axis=1, name='log_policy_for_action')\n",
    "                self.value_loss = tf.reduce_mean(tf.square(self.value - self.target_values), name='value_loss')\n",
    "                self.value_loss = self.value_loss * VALUE_MODIFIER\n",
    "                self.policy_loss = -tf.reduce_mean(tf.multiply(self.log_policy_for_action, self.advantages), name='policy_loss')\n",
    "                self.policy_loss = self.policy_loss * POLICY_MODIFIER\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_beta = tf.get_variable('entropy_beta', shape=[],\n",
    "                                       initializer=tf.constant_initializer(ENTROPY_MODIFIER), trainable=False)\n",
    "                self.entropy_loss = -tf.reduce_mean(self.policy * -self.log_policy, name='entropy_loss')\n",
    "                self.entropy_loss = self.entropy_loss * self.entropy_beta\n",
    "                self.loss = self.value_loss + \\\n",
    "                            self.policy_loss + \\\n",
    "                            self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                grads, grad_norms = tf.clip_by_global_norm(grads, CLIP_VALUE)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(grad, var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state, initial_lstm_state):\n",
    "        policy, final_lstm_state = sess.run((self.policy, self.final_lstm_state), \\\n",
    "                                            feed_dict={\\\n",
    "                                                       self.c_in:initial_lstm_state[0], \\\n",
    "                                                       self.h_in:initial_lstm_state[1], \\\n",
    "                                                       self.X:state\\\n",
    "                                                      }\\\n",
    "                                           )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction, final_lstm_state\n",
    "            \n",
    "    def act(self, sess, state, initial_lstm_state):\n",
    "        prediction, final_lstm_state = self.predict(sess, state, initial_lstm_state)\n",
    "        a = prediction\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state, final_lstm_state\n",
    "    \n",
    "    def get_value(self, sess, state, initial_lstm_state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.c_in:initial_lstm_state[0], \\\n",
    "                                   self.h_in:initial_lstm_state[1], \\\n",
    "                                   self.X:state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages, initial_lstm_state):\n",
    "        gu, value_loss, policy_loss, entropy_loss, final_lstm_state = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss, self.final_lstm_state), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages,\n",
    "                         self.c_in:initial_lstm_state[0], \\\n",
    "                         self.h_in:initial_lstm_state[1]\n",
    "                                   \n",
    "                     })\n",
    "        return value_loss / len(states), policy_loss / len(states), entropy_loss / len(states), final_lstm_state     \n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        global start_time\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        last_final_lstm_state = None\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "                self.agent.update_to_global(sess)\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                advantages = []\n",
    "                target_values = []\n",
    "                \n",
    "                initial_lstm_state = last_final_lstm_state\n",
    "                \n",
    "                if last_final_lstm_state == None:\n",
    "                        state_c = np.zeros((1, self.agent.lstm.state_size.c), np.float32)\n",
    "                        state_h = np.zeros((1, self.agent.lstm.state_size.h), np.float32)\n",
    "                        initial_lstm_state = [state_c, state_h]\n",
    "                \n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    val = self.agent.get_value(sess, s, initial_lstm_state)\n",
    "                    s, a, r, d, ns, initial_lstm_state = self.agent.act(sess, s, initial_lstm_state)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                    s = ns\n",
    "                \n",
    "                R = 0\n",
    "                if not done:\n",
    "                    R = last_values[-1]#self.agent.get_value(sess, s, initial_lstm_state)\n",
    "                \n",
    "                advantages = [0 for i in range(len(values))]\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    idx = len(rewards) - 1 - i\n",
    "                    reward = rewards[idx]\n",
    "                    R += DISCOUNT * reward\n",
    "                    advantage = (R - values[idx])\n",
    "                    advantages[idx] = advantage\n",
    "                    last_advantages.append(advantage)\n",
    "                    \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = last_values[-1]\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                states = np.vstack(states)\n",
    "                actions = np.vstack(actions).ravel()\n",
    "                target_values = np.vstack(target_values).ravel()\n",
    "                advantages = np.vstack(advantages).ravel()\n",
    "                \n",
    "                if last_final_lstm_state == None:\n",
    "                    state_c = np.zeros((1, self.agent.lstm.state_size.c), np.float32)\n",
    "                    state_h = np.zeros((1, self.agent.lstm.state_size.h), np.float32)\n",
    "                    last_final_lstm_state = [state_c, state_h]\n",
    "                \n",
    "                value_loss, policy_loss, entropy_loss, last_final_lstm_state = \\\n",
    "                    self.agent.train(sess, states, actions, target_values, advantages, last_final_lstm_state)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                \n",
    "                if done:\n",
    "                    last_final_lstm_state = None\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "#global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.GradientDescentOptimizer(LEARNING_RATE))\n",
    "\n",
    "config = tf.ConfigProto()#device_count = {'GPU': 0})\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    global global_counter\n",
    "    \n",
    "    MAX_MODELS = 1\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #save model every 15 minutes\n",
    "        while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            print(\"Current global iteration\", global_counter)\n",
    "            cnt_model += 1\n",
    "            time.sleep(15 * 60)\n",
    "        print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_threads = 20\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.25)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_fun(test_agent):\n",
    "    test_env = EnvWrapper(ENV_NAME)\n",
    "    #test_agent = Agent(test_env, 'tester', optimizer)\n",
    "    test_agent.update_to_global(sess)\n",
    "\n",
    "    state_c = np.zeros((1, test_agent.lstm.state_size.c), np.float32)\n",
    "    state_h = np.zeros((1, test_agent.lstm.state_size.h), np.float32)\n",
    "    initial_lstm_state = [state_c, state_h]\n",
    "\n",
    "    done = False\n",
    "    state = test_env.reset()\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        policy, final_lstm_state = sess.run((test_agent.policy, test_agent.final_lstm_state), \\\n",
    "                                                feed_dict={\\\n",
    "                                                           test_agent.c_in:initial_lstm_state[0], \\\n",
    "                                                           test_agent.h_in:initial_lstm_state[1], \\\n",
    "                                                           test_agent.X:state\\\n",
    "                                                          }\\\n",
    "                                               )\n",
    "        initial_lstm_state = final_lstm_state\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        prediction = np.random.choice(test_env.action_space.n, p=policy)\n",
    "        #if random.random() < 0.05:\n",
    "        #    prediction = test_env.action_space.sample()\n",
    "\n",
    "        ns, r, d, _ = test_env.step(prediction)\n",
    "        test_env.env.render()\n",
    "        state = ns\n",
    "        reward += (r)\n",
    "        done = d\n",
    "    test_env.env.close()\n",
    "    print('final reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from previous/lstm_2018_6_13/models/model_60.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_env = EnvWrapper(ENV_NAME)\n",
    "tester_agent = global_agent\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, 'previous/lstm_2018_6_13/models/model_60.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final reward is 48.0\n",
      "final reward is 21.0\n",
      "final reward is 28.0\n",
      "final reward is 29.0\n",
      "final reward is 44.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_agent_fun(tester_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
