{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 105\n",
    "IMG_HEIGHT = 80\n",
    "CNT_FRAMES = 4\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 0.5*1e0\n",
    "POLICY_MODIFIER = 1*1e0\n",
    "ENTROPY_MODIFIER = 1e-1#2.5e-5#0.0005\n",
    "MAX_STEPS = 30\n",
    "DISCOUNT = 0.99\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'\n",
    "MAX_EP_LENGTH = 1000\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIP_VALUE = 10.0\n",
    "DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.stack([s for i in range(CNT_FRAMES)], axis=3)\n",
    "        s = s.reshape(1, s.shape[1], s.shape[2], s.shape[3])\n",
    "        self.s = np.copy(s)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.append(s1, self.s[:, :, :, :CNT_FRAMES-1], axis=3)\n",
    "        self.s = np.copy(s)\n",
    "        return s, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iter = 0\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            #weights_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "            weights_initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_FRAMES], dtype=tf.float32, name='input')\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='first_conv')\n",
    "            mp1 = tf.contrib.layers.max_pool2d(conv1, 2, scope='first_mp')\n",
    "            conv2 = tf.contrib.layers.conv2d(mp1, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='second_conv')\n",
    "            mp2 = tf.contrib.layers.max_pool2d(conv2, 2, scope='second_mp')\n",
    "            conv3 = tf.contrib.layers.conv2d(mp2, 64, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                             weights_initializer=weights_initializer, biases_initializer = bias_initializer,\\\n",
    "                                            scope='third_conv')\n",
    "            flattened = tf.contrib.layers.flatten(conv3, scope='flatten')\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=tf.random_normal_initializer(stddev=0.02), biases_initializer=bias_initializer,\\\n",
    "                                                         scope='fc_embed')\n",
    "            \n",
    "            #normalization = tf.layers.batch_normalization(embedding)\n",
    "                        \n",
    "            self.policy = tf.contrib.layers.fully_connected(embedding, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=tf.random_normal_initializer(stddev=0.5), biases_initializer=None,\\\n",
    "                                                           scope='fc_policy')\n",
    "            self.value = tf.contrib.layers.fully_connected(\\\n",
    "                                                           embedding, \\\n",
    "                                                           1, \\\n",
    "                                                           activation_fn=None, \\\n",
    "                                                           weights_initializer=tf.random_normal_initializer(stddev=.25), \\\n",
    "                                                           biases_initializer=None,\\\n",
    "                                                          scope='fc_value')\n",
    "            \n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32, name='actions')\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32, name='actions_oh')\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32, name='target_vals')\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32, name='advantages')\n",
    "                #print('adv shape', self.advantages.shape)\n",
    "                #self.advantages = tf.subtract(tf.stop_gradient(self.value), self.target_values, name='advantage')\n",
    "\n",
    "                MIN_POLICY = 1e-8\n",
    "                MAX_POLICY = 1.0 - MIN_POLICY\n",
    "                \n",
    "                self.log_policy = tf.log(tf.clip_by_value(self.policy, MIN_POLICY, MAX_POLICY), name='log_policy')\n",
    "\n",
    "                self.log_policy_for_action = tf.reduce_sum(tf.multiply(self.log_policy, self.actions_oh), axis=1, name='log_policy_for_action')\n",
    "                self.value_loss = tf.reduce_mean(tf.square(self.value - self.target_values), name='value_loss')\n",
    "                self.value_loss = self.value_loss * VALUE_MODIFIER\n",
    "                #self.value_loss = self.value_loss - self.value_loss\n",
    "                self.policy_loss = -tf.reduce_mean(tf.multiply(self.log_policy_for_action, self.advantages), name='policy_loss')\n",
    "                self.policy_loss = self.policy_loss * POLICY_MODIFIER\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_beta = tf.get_variable('entropy_beta', shape=[],\n",
    "                                       initializer=tf.constant_initializer(ENTROPY_MODIFIER), trainable=False)\n",
    "                self.entropy_loss = -tf.reduce_mean(self.policy * -self.log_policy, name='entropy_loss')\n",
    "                self.entropy_loss = self.entropy_loss * self.entropy_beta\n",
    "                #self.entropy_loss = self.entropy_loss - self.entropy_loss\n",
    "                self.loss = self.value_loss + \\\n",
    "                            self.policy_loss + \\\n",
    "                            self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                grads = [tf.clip_by_average_norm(grad, CLIP_VALUE) for grad in grads]\n",
    "                #grads, grad_norms = tf.clip_by_global_norm(grads, CLIP_VALUE)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(grad, var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        policy = sess.run((self.policy), \\\n",
    "                                            feed_dict={\\\n",
    "                                                       self.X:state\\\n",
    "                                                      }\\\n",
    "                                           )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        \n",
    "        global last_iter\n",
    "        \n",
    "        if global_counter - 500000 > last_iter and self.scope_name == 'local0':\n",
    "            if hasattr(self, 'k5cnt') == False:\n",
    "                self.k5cnt = 0\n",
    "            self.k5cnt += 1\n",
    "            last_iter = global_counter\n",
    "            print('at iter', global_counter)\n",
    "        \n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction\n",
    "            \n",
    "    def act(self, sess, state):\n",
    "        prediction = self.predict(sess, state)\n",
    "        a = prediction\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state\n",
    "    \n",
    "    def get_value(self, sess, state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.X: state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages):\n",
    "        gu, value_loss, policy_loss, entropy_loss = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages\n",
    "                     })\n",
    "        return value_loss / len(states), policy_loss / len(states), entropy_loss / len(states)\n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        global start_time\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    self.agent.update_to_global(sess)\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                target_values = []\n",
    "                \n",
    "                has_rewards = False\n",
    "                \n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    s, a, r, d, ns = self.agent.act(sess, s)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                    r = np.clip(r, -1.0, 1.0)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    val = np.copy(self.agent.get_value(sess, s)[0])\n",
    "                    #print('val', val)\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                    \n",
    "                    s = ns\n",
    "                    \n",
    "                    #has_rewards = has_rewards or r != 0.0\n",
    "                \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = values[-1]\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                target_values.reverse()\n",
    "                \n",
    "                #advantages = np.array(target_values) - np.array(values)\n",
    "                \n",
    "                #print('values shape', np.array(values).shape)\n",
    "                #print('target values shape', np.array(target_values).shape)\n",
    "                advantages = np.array(target_values).flatten() - np.array(values).flatten()\n",
    "                advantages = advantages.flatten()\n",
    "                \n",
    "                #print('adv before app', advantages.shape)\n",
    "                \n",
    "                states = np.vstack(states)\n",
    "                #print('states shape', states.shape)\n",
    "                actions = np.array(actions).flatten()\n",
    "                #print('actions shape', actions.shape)\n",
    "                target_values = np.array(target_values).flatten()\n",
    "                \n",
    "                 #np.vstack(advantages).ravel()\n",
    "                \n",
    "                value_loss, policy_loss, entropy_loss = \\\n",
    "                    self.agent.train(sess, states, actions, target_values, advantages)\n",
    "                \n",
    "                last_advantages += advantages.tolist()\n",
    "                \n",
    "                #if has_rewards:\n",
    "                    #print('rewarded round')\n",
    "                    #print('target values', target_values)\n",
    "                    #print('values', np.array(values).flatten())\n",
    "                    #print('advantages', advantages)\n",
    "                    #print('value loss', value_loss)\n",
    "                    #print('policy loss', policy_loss)\n",
    "                    #print('entropy loss', entropy_loss)\n",
    "                \n",
    "                #if self.agent.scope_name == 'local0':\n",
    "                #    print('values', values)\n",
    "                #    print('target values', target_values)\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                               \n",
    "                #sleep_time = random.uniform(1e-3, 1e-2)\n",
    "                #time.sleep(sleep_time)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "#global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.GradientDescentOptimizer(LEARNING_RATE))\n",
    "\n",
    "#config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    global global_counter\n",
    "    \n",
    "    MAX_MODELS = 3\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #save model every 15 minutes\n",
    "        while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            print(\"Current global iteration\", global_counter)\n",
    "            cnt_model += 1\n",
    "            time.sleep(30 * 60)\n",
    "        print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "building agent: local0\n",
      "building model\n",
      "building agent: local1\n",
      "building model\n",
      "building agent: local2\n",
      "building model\n",
      "building agent: local3\n",
      "building model\n",
      "building agent: local4\n",
      "building model\n",
      "building agent: local5\n",
      "building model\n",
      "building agent: local6\n",
      "building model\n",
      "building agent: local7\n",
      "building model\n",
      "building agent: local8\n",
      "building model\n",
      "building agent: local9\n",
      "building model\n",
      "building agent: local10\n",
      "building model\n",
      "building agent: local11\n",
      "building model\n",
      "building agent: local12\n",
      "building model\n",
      "building agent: local13\n",
      "building model\n",
      "building agent: local14\n",
      "building model\n",
      "building agent: local15\n",
      "worker starting agent: local0\n",
      "worker starting agent: local1\n",
      "worker starting agent: local2\n",
      "worker starting agent: local3\n",
      "worker starting agent: local4\n",
      "worker starting agent: local5\n",
      "worker starting agent: local6\n",
      "worker starting agent: local7\n",
      "worker starting agent: local8\n",
      "worker starting agent: local9\n",
      "worker starting agent: local10\n",
      "worker starting agent: local11\n",
      "worker starting agent: local12\n",
      "worker starting agent: local13\n",
      "worker starting agent: local14\n",
      "worker starting agent: local15\n",
      "Current model save name: model_0\n",
      "Current global iteration 16\n",
      "at iter 500019\n",
      "Current model save name: model_1\n",
      "Current global iteration 658233\n",
      "at iter 1000020\n",
      "Current model save name: model_2\n",
      "Current global iteration 1326882\n",
      "at iter 1500032\n",
      "Current model save name: model_0\n",
      "Current global iteration 1996395\n",
      "at iter 2000056\n",
      "at iter 2500065\n",
      "Current model save name: model_1\n",
      "Current global iteration 2662388\n",
      "at iter 3000075\n",
      "Current model save name: model_2\n",
      "Current global iteration 3332542\n",
      "at iter 3500080\n",
      "at iter 4000083\n",
      "Current model save name: model_0\n",
      "Current global iteration 4000642\n",
      "at iter 4500093\n",
      "Current model save name: model_1\n",
      "Current global iteration 4671048\n",
      "at iter 5000106\n",
      "Current model save name: model_2\n",
      "Current global iteration 5338540\n",
      "at iter 5500108\n",
      "at iter 6000111\n",
      "Current model save name: model_0\n",
      "Current global iteration 6006671\n",
      "at iter 6500124\n",
      "Current model save name: model_1\n",
      "Current global iteration 6675114\n",
      "at iter 7000128\n",
      "Current model save name: model_2\n",
      "Current global iteration 7342913\n",
      "at iter 7500147\n",
      "at iter 8000152\n",
      "Current model save name: model_0\n",
      "Current global iteration 8009029\n",
      "at iter 8500164\n",
      "Current model save name: model_1\n",
      "Current global iteration 8676025\n",
      "at iter 9000174\n",
      "Current model save name: model_2\n",
      "Current global iteration 9344462\n",
      "at iter 9500191\n",
      "at iter 10000195\n",
      "Current model save name: model_0\n",
      "Current global iteration 10012272\n",
      "at iter 10500236\n",
      "Current model save name: model_1\n",
      "Current global iteration 10681811\n",
      "at iter 11000239\n",
      "Current model save name: model_2\n",
      "Current global iteration 11344475\n",
      "at iter 11500242\n",
      "at iter 12000258\n",
      "Current model save name: model_0\n",
      "Current global iteration 12002461\n",
      "at iter 12500267\n",
      "Current model save name: model_1\n",
      "Current global iteration 12652875\n",
      "at iter 13000277\n",
      "Current model save name: model_2\n",
      "Current global iteration 13298073\n",
      "at iter 13500291\n",
      "Current model save name: model_0\n",
      "Current global iteration 13967266\n",
      "at iter 14000299\n",
      "at iter 14500301\n",
      "Current model save name: model_1\n",
      "Current global iteration 14633850\n",
      "at iter 15000310\n",
      "Current model save name: model_2\n",
      "Current global iteration 15303783\n",
      "at iter 15500320\n",
      "Current model save name: model_0\n",
      "Current global iteration 15971495\n",
      "at iter 16000331\n",
      "at iter 16500342\n",
      "Current model save name: model_1\n",
      "Current global iteration 16640779\n",
      "at iter 17000351\n",
      "Current model save name: model_2\n",
      "Current global iteration 17309991\n",
      "at iter 17500364\n",
      "Current model save name: model_0\n",
      "Current global iteration 17978907\n",
      "at iter 18000377\n",
      "at iter 18500381\n",
      "Current model save name: model_1\n",
      "Current global iteration 18646414\n",
      "at iter 19000390\n",
      "Current model save name: model_2\n",
      "Current global iteration 19305998\n",
      "at iter 19500428\n",
      "Current model save name: model_0\n",
      "Current global iteration 19971802\n",
      "at iter 20000439\n",
      "at iter 20500444\n",
      "Current model save name: model_1\n",
      "Current global iteration 20639491\n",
      "at iter 21000450\n",
      "Current model save name: model_2\n",
      "Current global iteration 21306923\n",
      "at iter 21500452\n",
      "Current model save name: model_0\n",
      "Current global iteration 21975339\n",
      "at iter 22000458\n",
      "at iter 22500476\n",
      "Current model save name: model_1\n",
      "Current global iteration 22644846\n",
      "at iter 23000483\n",
      "Current model save name: model_2\n",
      "Current global iteration 23312620\n",
      "at iter 23500487\n",
      "Current model save name: model_0\n",
      "Current global iteration 23981636\n",
      "at iter 24000503\n",
      "at iter 24500520\n",
      "Current model save name: model_1\n",
      "Current global iteration 24649835\n",
      "at iter 25000521\n",
      "Current model save name: model_2\n",
      "Current global iteration 25316784\n",
      "at iter 25500527\n",
      "Current model save name: model_0\n",
      "Current global iteration 25984433\n",
      "at iter 26000533\n",
      "at iter 26500547\n",
      "Current model save name: model_1\n",
      "Current global iteration 26652451\n",
      "at iter 27000563\n",
      "Current model save name: model_2\n",
      "Current global iteration 27320270\n",
      "at iter 27500566\n",
      "Current model save name: model_0\n",
      "Current global iteration 27986948\n",
      "at iter 28000580\n",
      "at iter 28500590\n",
      "Current model save name: model_1\n",
      "Current global iteration 28655688\n",
      "at iter 29000594\n",
      "Current model save name: model_2\n",
      "Current global iteration 29321912\n",
      "at iter 29500596\n",
      "Current model save name: model_0\n",
      "Current global iteration 29991378\n",
      "at iter 30000601\n",
      "at iter 30500616\n",
      "Current model save name: model_1\n",
      "Current global iteration 30658784\n",
      "at iter 31000622\n",
      "Current model save name: model_2\n",
      "Current global iteration 31325180\n",
      "at iter 31500633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-16f06c2360ba>\", line 27, in <module>\n",
      "    t.join()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 718, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 372, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 406, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 636, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 656, in _abort_queue\n",
      "    content=status, parent=msg, ident=idents)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/jupyter_client/session.py\", line 748, in send\n",
      "    stream.send_multipart(to_send, copy=copy)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 272, in send_multipart\n",
      "    self._add_io_state(zmq.POLLOUT)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 521, in _add_io_state\n",
      "    self._update_handler(self._state)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in _update_handler\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 134, in add_callback\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 535, in call_soon_threadsafe\n",
      "    self._write_to_self()\n",
      "  File \"/usr/lib/python3.5/asyncio/selector_events.py\", line 146, in _write_to_self\n",
      "    csock.send(b'\\0')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "cnt_threads = 16\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE, decay=DECAY)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_fun(test_agent):\n",
    "    test_env = EnvWrapper(ENV_NAME)\n",
    "    #test_agent = Agent(test_env, 'tester', optimizer)\n",
    "    #test_agent.update_to_global(sess)\n",
    "\n",
    "    done = False\n",
    "    state = test_env.reset()\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        policy = sess.run((test_agent.policy), \\\n",
    "                                                feed_dict={\\\n",
    "                                                           test_agent.X:state\\\n",
    "                                                          }\\\n",
    "                                               )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        prediction = np.random.choice(test_env.action_space.n, p=policy)\n",
    "        \n",
    "        ns, r, d, _ = test_env.step(prediction)\n",
    "        #test_env.env.render()\n",
    "        state = ns\n",
    "        reward += (r)\n",
    "        done = d\n",
    "    test_env.env.close()\n",
    "    print('final reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_0.ckpt\n",
      "final reward is 47.0\n",
      "final reward is 42.0\n",
      "final reward is 83.0\n",
      "final reward is 44.0\n",
      "final reward is 93.0\n",
      "final reward is 65.0\n",
      "final reward is 35.0\n",
      "final reward is 70.0\n",
      "final reward is 87.0\n",
      "final reward is 76.0\n"
     ]
    }
   ],
   "source": [
    "#tester_agent = global_agent\n",
    "#saver = tf.train.Saver()\n",
    "tester_agent = global_agent\n",
    "with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'models/model_0.ckpt')\n",
    "    for i in range(10):\n",
    "        test_agent_fun(tester_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
