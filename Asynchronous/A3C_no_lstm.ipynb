{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "CNT_FRAMES = 4\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 5\n",
    "POLICY_MODIFIER = 10\n",
    "ENTROPY_MODIFIER = 1e-4#0.0005\n",
    "MAX_STEPS = 30\n",
    "DISCOUNT = 0.99\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'\n",
    "MAX_EP_LENGTH = 500\n",
    "LEARNING_RATE = 2.5*1e-4\n",
    "CLIP_VALUE = 0.1\n",
    "DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.stack([s for i in range(CNT_FRAMES)], axis=3)\n",
    "        s = s.reshape(1, s.shape[1], s.shape[2], s.shape[3])\n",
    "        self.s = np.copy(s)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.append(s1, self.s[:, :, :, :CNT_FRAMES-1], axis=3)\n",
    "        self.s = np.copy(s)\n",
    "        return s, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iter = 0\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            weights_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_FRAMES], dtype=tf.float32)\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            mp1 = tf.contrib.layers.max_pool2d(conv1, 2)\n",
    "            conv2 = tf.contrib.layers.conv2d(mp1, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            mp2 = tf.contrib.layers.max_pool2d(conv2, 2)\n",
    "            conv3 = tf.contrib.layers.conv2d(mp2, 64, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME', \\\n",
    "                                             weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=tf.random_normal_initializer(stddev=0.02), biases_initializer=bias_initializer)\n",
    "            \n",
    "            #normalization = tf.layers.batch_normalization(embedding)\n",
    "                        \n",
    "            self.policy = tf.contrib.layers.fully_connected(embedding, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=tf.random_normal_initializer(stddev=0.5), biases_initializer=None)\n",
    "            self.value = tf.contrib.layers.fully_connected(embedding, 1, activation_fn=None, weights_initializer=normalized_columns_initializer(1.), biases_initializer=None)\n",
    "            \n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32)\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                #self.advantages = tf.subtract(tf.stop_gradient(self.value), self.target_values, name='advantage')\n",
    "\n",
    "                MIN_POLICY = 1e-8\n",
    "                MAX_POLICY = 1.0 - MIN_POLICY\n",
    "                \n",
    "                self.log_policy = tf.log(tf.clip_by_value(self.policy, MIN_POLICY, MAX_POLICY))\n",
    "\n",
    "                self.log_policy_for_action = tf.reduce_sum(self.log_policy * self.actions_oh, axis=1)\n",
    "                self.value_loss = tf.reduce_mean(tf.squared_difference(self.value, self.target_values))\n",
    "                self.policy_loss = -tf.reduce_mean(self.log_policy_for_action * self.advantages)\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_loss = -tf.reduce_mean(self.policy * -self.log_policy)\n",
    "                #self.entropy_loss = self.entropy_loss - self.entropy_loss\n",
    "                self.loss = VALUE_MODIFIER * self.value_loss + \\\n",
    "                            POLICY_MODIFIER * self.policy_loss + \\\n",
    "                            ENTROPY_MODIFIER * self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                grads = [tf.clip_by_average_norm(grad, CLIP_VALUE) for grad in grads]\n",
    "                #grads, grad_norms = tf.clip_by_global_norm(grads, CLIP_VALUE)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(grad, var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        policy = sess.run((self.policy), \\\n",
    "                                            feed_dict={\\\n",
    "                                                       self.X:state\\\n",
    "                                                      }\\\n",
    "                                           )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        \n",
    "        global last_iter\n",
    "        \n",
    "        if global_counter - 5000 > last_iter:\n",
    "            last_iter = global_counter\n",
    "            print('policy', policy, 'at iter', global_counter)\n",
    "        \n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction\n",
    "            \n",
    "    def act(self, sess, state):\n",
    "        prediction = self.predict(sess, state)\n",
    "        a = prediction\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state\n",
    "    \n",
    "    def get_value(self, sess, state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.X: state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages):\n",
    "        gu, value_loss, policy_loss, entropy_loss = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages\n",
    "                     })\n",
    "        return value_loss / len(states), policy_loss / len(states), entropy_loss / len(states)\n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        global start_time\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    self.agent.update_to_global(sess)\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                advantages = []\n",
    "                target_values = []\n",
    "                \n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    s, a, r, d, ns = self.agent.act(sess, s)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                    r = np.clip(r, -1.0, 1.0)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    val = self.agent.get_value(sess, s)\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                \n",
    "                R = 0\n",
    "                if not done:\n",
    "                    R = self.agent.get_value(sess, s)\n",
    "                \n",
    "                advantages = [0 for i in range(len(values))]\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    idx = len(rewards) - 1 - i\n",
    "                    reward = rewards[idx]\n",
    "                    R += DISCOUNT * reward\n",
    "                    advantage = (R - values[idx])\n",
    "                    advantages[idx] = advantage\n",
    "                    last_advantages.append(advantage)\n",
    "                    \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = self.agent.get_value(sess, s)\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                target_values.reverse()\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                states = np.vstack(states)\n",
    "                actions = np.vstack(actions).ravel()\n",
    "                target_values = np.vstack(target_values).ravel()\n",
    "                advantages = np.vstack(advantages).ravel()\n",
    "                \n",
    "                value_loss, policy_loss, entropy_loss = \\\n",
    "                    self.agent.train(sess, states, actions, target_values, advantages)\n",
    "                \n",
    "                sleep_time = random.uniform(1e-3, 1e-2)\n",
    "                time.sleep(sleep_time)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "#global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.GradientDescentOptimizer(LEARNING_RATE))\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    global global_counter\n",
    "    \n",
    "    MAX_MODELS = 1\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #save model every 15 minutes\n",
    "        while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            print(\"Current global iteration\", global_counter)\n",
    "            cnt_model += 1\n",
    "            time.sleep(15 * 60)\n",
    "        print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "building agent: local0\n",
      "building model\n",
      "building agent: local1\n",
      "building model\n",
      "building agent: local2\n",
      "building model\n",
      "building agent: local3\n",
      "building model\n",
      "building agent: local4\n",
      "building model\n",
      "building agent: local5\n",
      "building model\n",
      "building agent: local6\n",
      "building model\n",
      "building agent: local7\n",
      "building model\n",
      "building agent: local8\n",
      "building model\n",
      "building agent: local9\n",
      "building model\n",
      "building agent: local10\n",
      "building model\n",
      "building agent: local11\n",
      "building model\n",
      "building agent: local12\n",
      "building model\n",
      "building agent: local13\n",
      "building model\n",
      "building agent: local14\n",
      "building model\n",
      "building agent: local15\n",
      "worker starting agent: local0\n",
      "worker starting agent: local1\n",
      "worker starting agent: local2\n",
      "worker starting agent: local3\n",
      "worker starting agent: local4\n",
      "worker starting agent: local5\n",
      "worker starting agent: local6\n",
      "worker starting agent: local7\n",
      "worker starting agent: local8\n",
      "worker starting agent: local9\n",
      "worker starting agent: local10\n",
      "worker starting agent: local11\n",
      "worker starting agent: local12\n",
      "worker starting agent: local13\n",
      "worker starting agent: local14\n",
      "worker starting agent: local15\n",
      "Current model save name: model_0\n",
      "Current global iteration 27\n",
      "policy [0.22396187 0.2788532  0.2432905  0.25389442] at iter 5003\n",
      "policy [0.20586123 0.22732389 0.28562137 0.28119346] at iter 10004\n",
      "policy [0.15685359 0.15168396 0.29833728 0.39312518] at iter 15007\n",
      "policy [0.08489225 0.02174425 0.26847643 0.6248871 ] at iter 20008\n",
      "policy [0.0887455  0.022842   0.25960574 0.6288067 ] at iter 25011\n",
      "policy [0.23452893 0.20435275 0.25674358 0.30437472] at iter 30012\n",
      "policy [0.29042077 0.18513846 0.2460388  0.27840203] at iter 35021\n",
      "policy [0.29109734 0.18950748 0.21943656 0.29995862] at iter 40026\n",
      "policy [0.251048   0.20645425 0.20712414 0.3353736 ] at iter 45027\n"
     ]
    }
   ],
   "source": [
    "cnt_threads = 16\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE, decay=DECAY)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_fun(test_agent):\n",
    "    test_env = EnvWrapper(ENV_NAME)\n",
    "    #test_agent = Agent(test_env, 'tester', optimizer)\n",
    "    test_agent.update_to_global(sess)\n",
    "\n",
    "    done = False\n",
    "    state = test_env.reset()\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        policy = sess.run((test_agent.policy), \\\n",
    "                                                feed_dict={\\\n",
    "                                                           test_agent.X:state\\\n",
    "                                                          }\\\n",
    "                                               )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        prediction = np.random.choice(test_env.action_space.n, p=policy)\n",
    "        if random.random() < 0.05:\n",
    "            prediction = env.action_space.sample()\n",
    "\n",
    "        ns, r, d, _ = test_env.step(prediction)\n",
    "        test_env.env.render()\n",
    "        state = ns\n",
    "        reward += (r)\n",
    "        done = d\n",
    "    test_env.env.close()\n",
    "    print('final reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = EnvWrapper(ENV_NAME)\n",
    "tester_agent = global_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_fun(tester_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
