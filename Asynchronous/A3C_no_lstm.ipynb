{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import color, exposure, transform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "CNT_FRAMES = 4\n",
    "GLOBAL_SCOPE = 'global'\n",
    "VALUE_MODIFIER = 0.25\n",
    "POLICY_MODIFIER = 1\n",
    "ENTROPY_MODIFIER = 2.5 * 1e-4#0.0005\n",
    "MAX_STEPS = 10\n",
    "DISCOUNT = 0.99\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'\n",
    "MAX_EP_LENGTH = 500\n",
    "LEARNING_RATE = 2.5*1e-4\n",
    "CLIP_VALUE = 1\n",
    "DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(x_t, img_rows, img_cols):\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t,(img_rows, img_cols), mode='constant')\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "    x_t = x_t.reshape((1, img_rows, img_cols, 1))\n",
    "    x_t /= 255.0\n",
    "    return x_t\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_space = self.env.action_space\n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = process_frame(s, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.stack([s for i in range(CNT_FRAMES)], axis=3)\n",
    "        s = s.reshape(1, s.shape[1], s.shape[2], s.shape[3])\n",
    "        self.s = np.copy(s)\n",
    "        return s\n",
    "    def step(self, a):\n",
    "        s1, r, d, _ = self.env.step(a)\n",
    "        s1 = process_frame(s1, IMG_WIDTH, IMG_HEIGHT)\n",
    "        s = np.append(s1, self.s[:, :, :, :CNT_FRAMES-1], axis=3)\n",
    "        self.s = np.copy(s)\n",
    "        return s, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iter = 0\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, scope_name, optimizer):\n",
    "        self.env = env\n",
    "        self.scope_name = scope_name\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.__build_model()\n",
    "    def __build_model(self):\n",
    "        print('building model')\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            weights_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, CNT_FRAMES], dtype=tf.float32)\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                            weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            conv3 = tf.contrib.layers.conv2d(conv2, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                             weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            conv4 = tf.contrib.layers.conv2d(conv3, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='VALID', \\\n",
    "                                             weights_initializer=weights_initializer, biases_initializer = bias_initializer)\n",
    "            flattened = tf.contrib.layers.flatten(conv4)\n",
    "            embedding = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=tf.random_normal_initializer(stddev=0.02), biases_initializer=bias_initializer)\n",
    "            embedding2 = tf.contrib.layers.fully_connected(embedding, 128, activation_fn=tf.nn.relu, weights_initializer=tf.random_normal_initializer(stddev=0.02), biases_initializer=bias_initializer)\n",
    "            \n",
    "            #normalization = tf.layers.batch_normalization(embedding)\n",
    "                        \n",
    "            self.policy = tf.contrib.layers.fully_connected(embedding2, self.action_size, activation_fn=tf.nn.softmax, weights_initializer=tf.random_normal_initializer(stddev=0.5), biases_initializer=None)\n",
    "            self.value = tf.contrib.layers.fully_connected(embedding2, 1, activation_fn=None, weights_initializer=normalized_columns_initializer(1.), biases_initializer=None)\n",
    "            \n",
    "            if self.scope_name != GLOBAL_SCOPE:\n",
    "                print('building agent:', self.scope_name)\n",
    "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "                self.actions_oh = tf.one_hot(self.actions, depth=self.action_size, dtype=tf.float32)\n",
    "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "                MIN_POLICY = 1e-8\n",
    "                MAX_POLICY = 1.0 - MIN_POLICY\n",
    "                \n",
    "                self.log_policy = tf.log(tf.clip_by_value(self.policy, MIN_POLICY, MAX_POLICY))\n",
    "\n",
    "                self.log_policy_for_action = tf.reduce_sum(self.log_policy * self.actions_oh, axis=1)\n",
    "                self.value_loss = tf.reduce_mean(tf.squared_difference(self.value, self.target_values))\n",
    "                self.policy_loss = -tf.reduce_mean(self.log_policy_for_action * self.advantages)\n",
    "                #entropija je E[-log(X)] = sum(p(x) * log(x))\n",
    "                self.entropy_loss = -tf.reduce_mean(self.policy * -self.log_policy)\n",
    "                #self.entropy_loss = self.entropy_loss - self.entropy_loss\n",
    "                self.loss = VALUE_MODIFIER * self.value_loss + \\\n",
    "                            POLICY_MODIFIER * self.policy_loss + \\\n",
    "                            ENTROPY_MODIFIER * self.entropy_loss\n",
    "                #get locals\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "                #update locals\n",
    "                grads = tf.gradients(self.loss, local_vars)\n",
    "                grads, grad_norms = tf.clip_by_global_norm(grads, CLIP_VALUE)\n",
    "                self.update_ops = update_target_graph(GLOBAL_SCOPE, self.scope_name)\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, GLOBAL_SCOPE)\n",
    "                capped_gvs = [(grad, var) for grad, var in zip(grads, global_vars)]\n",
    "                self.global_update = self.optimizer.apply_gradients(capped_gvs)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        policy = sess.run((self.policy), \\\n",
    "                                            feed_dict={\\\n",
    "                                                       self.X:state\\\n",
    "                                                      }\\\n",
    "                                           )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        \n",
    "        global last_iter\n",
    "        \n",
    "        if global_counter - 5000 > last_iter:\n",
    "            last_iter = global_counter\n",
    "            print('policy', policy, 'at iter', global_counter)\n",
    "        \n",
    "        prediction = np.random.choice(self.action_size, p=policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        #print('prediction', prediction)\n",
    "        return prediction\n",
    "            \n",
    "    def act(self, sess, state):\n",
    "        prediction = self.predict(sess, state)\n",
    "        a = prediction\n",
    "        next_state,r,d,_ = self.env.step(a)\n",
    "        return state, a, r, d, next_state\n",
    "    \n",
    "    def get_value(self, sess, state):\n",
    "        return sess.run(\\\n",
    "                        self.value, \\\n",
    "                        feed_dict={ \\\n",
    "                                   self.X: state\\\n",
    "                                  } \\\n",
    "                       )\n",
    "    \n",
    "    def train(self, sess, states, actions, target_values, advantages):\n",
    "        gu, value_loss, policy_loss, entropy_loss = \\\n",
    "            sess.run((self.global_update, self.value_loss, self.policy_loss, self.entropy_loss), \\\n",
    "                     feed_dict={\n",
    "                         self.X: states,\n",
    "                         self.actions: actions,\n",
    "                         self.target_values: target_values,\n",
    "                         self.advantages: advantages\n",
    "                     })\n",
    "        return value_loss / len(states), policy_loss / len(states), entropy_loss / len(states)\n",
    "    \n",
    "    def update_to_global(self, sess):\n",
    "        if self.scope_name != GLOBAL_SCOPE:\n",
    "            sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.summary_writer = tf.summary.FileWriter(self.agent.scope_name)\n",
    "    def work(self, sess, optimizer, thread_lock):\n",
    "        \n",
    "        global global_counter\n",
    "        global start_time\n",
    "        \n",
    "        print('worker starting agent:', self.agent.scope_name)\n",
    "        done = True\n",
    "        s = None\n",
    "        episode_reward = 0\n",
    "        timestep = 0\n",
    "        episode_counter = 0\n",
    "        value_losses = []\n",
    "        policy_losses = []\n",
    "        entropy_losses = []\n",
    "        last_rewards = []\n",
    "        last_frames = []\n",
    "        last_values = []\n",
    "        last_advantages = []\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "                if done or timestep > MAX_EP_LENGTH:\n",
    "                    self.agent.update_to_global(sess)\n",
    "                    last_rewards.append(episode_reward)\n",
    "                    last_frames.append(timestep)\n",
    "                    if episode_counter > 0 and episode_counter % 5 == 0:\n",
    "                        #print('for agent:', self.agent.scope_name)\n",
    "                        #print('at episode', episode_counter, 'episode reward is', episode_reward)\n",
    "                        if len(value_losses) > 0:\n",
    "                            summary = tf.Summary()\n",
    "                            \n",
    "                            summary.value.add(tag='Performance/Reward', simple_value=float(sum(last_rewards) / len(last_rewards)))\n",
    "                            summary.value.add(tag='Performance/Length', simple_value=float(sum(last_frames) / len(last_frames)))\n",
    "                            summary.value.add(tag='Performance/Values mean', simple_value=float(sum(last_values) / len(last_values)))\n",
    "                            summary.value.add(tag='Performance/Advantage mean', simple_value=float(sum(last_advantages) / len(last_advantages)))\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(sum(value_losses) / len(value_losses)))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(sum(policy_losses) / len(policy_losses)))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(sum(entropy_losses) / len(entropy_losses)))\n",
    "                            \n",
    "                            self.summary_writer.add_summary(summary, episode_counter)\n",
    "\n",
    "                            self.summary_writer.flush()\n",
    "                            \n",
    "                            last_rewards = []\n",
    "                            last_frames = []\n",
    "                            value_losses = []\n",
    "                            policy_losses = []\n",
    "                            entropy_losses = []\n",
    "                            last_values = []\n",
    "                            last_advantages = []\n",
    "                    s = self.agent.env.reset()\n",
    "                    done = False\n",
    "                    episode_reward = 0\n",
    "                    timestep = 0\n",
    "                    episode_counter += 1\n",
    "                    \n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                values = []\n",
    "                advantages = []\n",
    "                target_values = []\n",
    "                \n",
    "                while len(states) < MAX_STEPS and not done:\n",
    "                    s, a, r, d, ns = self.agent.act(sess, s)\n",
    "                    with thread_lock:\n",
    "                        global_counter += 1\n",
    "                    episode_reward += r\n",
    "                    timestep += 1\n",
    "                    r = np.clip(r, -1.0, 1.0)\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    done = d\n",
    "                    val = self.agent.get_value(sess, s)\n",
    "                    last_values.append(val)\n",
    "                    values.append(val)\n",
    "                \n",
    "                R = 0\n",
    "                if not done:\n",
    "                    R = self.agent.get_value(sess, s)\n",
    "                \n",
    "                advantages = [0 for i in range(len(values))]\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    idx = len(rewards) - 1 - i\n",
    "                    reward = rewards[idx]\n",
    "                    R += DISCOUNT * reward\n",
    "                    advantage = (R - values[idx])\n",
    "                    advantages[idx] = advantage\n",
    "                    last_advantages.append(advantage)\n",
    "                    \n",
    "                target_value = 0\n",
    "                \n",
    "                if not done:\n",
    "                    target_value = self.agent.get_value(sess, s)\n",
    "                \n",
    "                for reward in reversed(rewards):\n",
    "                    target_value = reward + DISCOUNT * target_value\n",
    "                    target_values.append(target_value)\n",
    "                #for i in range(len(rewards)-1):\n",
    "                #    idx = len(rewards) - i - 1\n",
    "                #    target_values[idx-1] = rewards[idx-1] + DISCOUNT * target_values[idx]\n",
    "                states = np.vstack(states)\n",
    "                actions = np.vstack(actions).ravel()\n",
    "                target_values = np.vstack(target_values).ravel()\n",
    "                advantages = np.vstack(advantages).ravel()\n",
    "                \n",
    "                value_loss, policy_loss, entropy_loss = \\\n",
    "                    self.agent.train(sess, states, actions, target_values, advantages)\n",
    "                \n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "env_global = EnvWrapper(ENV_NAME)\n",
    "#global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.AdamOptimizer())\n",
    "global_agent = Agent(env_global, GLOBAL_SCOPE, tf.train.GradientDescentOptimizer(LEARNING_RATE))\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def global_saving_thread(agent, sess):\n",
    "    \n",
    "    global global_counter\n",
    "    \n",
    "    MAX_MODELS = 1\n",
    "    cnt_model = 0\n",
    "    \n",
    "    with sess.as_default(), sess.graph.as_default():\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #save model every 15 minutes\n",
    "        while True:#global_counter <= MAX_ITERATIONS and elapsed_time <= MAX_LEARNING_TIME:\n",
    "            print(\"Current model save name:\", 'model_' + str(cnt_model % MAX_MODELS))\n",
    "            save_path = saver.save(sess, \"models/model_\" + str(cnt_model % MAX_MODELS) + \".ckpt\")\n",
    "            print(\"Current global iteration\", global_counter)\n",
    "            cnt_model += 1\n",
    "            time.sleep(15 * 60)\n",
    "        print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "building agent: local0\n",
      "building model\n",
      "building agent: local1\n",
      "building model\n",
      "building agent: local2\n",
      "building model\n",
      "building agent: local3\n",
      "building model\n",
      "building agent: local4\n",
      "building model\n",
      "building agent: local5\n",
      "building model\n",
      "building agent: local6\n",
      "building model\n",
      "building agent: local7\n",
      "building model\n",
      "building agent: local8\n",
      "building model\n",
      "building agent: local9\n",
      "building model\n",
      "building agent: local10\n",
      "building model\n",
      "building agent: local11\n",
      "building model\n",
      "building agent: local12\n",
      "building model\n",
      "building agent: local13\n",
      "building model\n",
      "building agent: local14\n",
      "building model\n",
      "building agent: local15\n",
      "building model\n",
      "building agent: local16\n",
      "building model\n",
      "building agent: local17\n",
      "building model\n",
      "building agent: local18\n",
      "building model\n",
      "building agent: local19\n",
      "worker starting agent: local0\n",
      "worker starting agent: local1\n",
      "worker starting agent: local2\n",
      "worker starting agent: local3\n",
      "worker starting agent: local4\n",
      "worker starting agent: local5\n",
      "worker starting agent: local6\n",
      "worker starting agent: local7\n",
      "worker starting agent: local8\n",
      "worker starting agent: local9\n",
      "worker starting agent: local10\n",
      "worker starting agent: local11\n",
      "worker starting agent: local12\n",
      "worker starting agent: local13\n",
      "worker starting agent: local14\n",
      "worker starting agent: local15\n",
      "worker starting agent: local16\n",
      "worker starting agent: local17\n",
      "worker starting agent: local18\n",
      "worker starting agent: local19\n",
      "Current model save name: model_0\n",
      "Current global iteration 66\n",
      "policy [0.23276667 0.24015008 0.25116354 0.27591974] at iter 5001\n",
      "policy [0.2521507  0.23799466 0.259711   0.25014365] at iter 10007\n",
      "policy [0.22995812 0.28789768 0.20910132 0.27304295] at iter 15009\n",
      "policy [0.24383111 0.3272456  0.14133762 0.28758574] at iter 20010\n",
      "policy [0.11673126 0.62794715 0.10964831 0.14567323] at iter 25014\n",
      "policy [0.26039076 0.24105734 0.24242342 0.2561284 ] at iter 30016\n",
      "policy [0.27491552 0.22787608 0.24004386 0.25716457] at iter 35018\n",
      "policy [0.16519555 0.14325763 0.3043248  0.38722205] at iter 40020\n",
      "policy [0.13994868 0.10519547 0.351354   0.40350184] at iter 45023\n",
      "policy [0.08981853 0.05481139 0.28008914 0.5752809 ] at iter 50025\n",
      "policy [0.08981853 0.05481139 0.28008914 0.5752809 ] at iter 55027\n",
      "policy [0.05929234 0.03102728 0.24241011 0.66727024] at iter 60028\n",
      "policy [0.0218025  0.02389045 0.11947358 0.8348335 ] at iter 65030\n",
      "policy [0.04193364 0.05223941 0.21646145 0.6893655 ] at iter 70031\n",
      "policy [0.01354238 0.01899151 0.14125954 0.8262066 ] at iter 75033\n",
      "policy [0.00754981 0.01344539 0.10078023 0.8782246 ] at iter 80035\n",
      "policy [0.00443609 0.00847673 0.07002072 0.9170665 ] at iter 85043\n",
      "policy [0.00443609 0.00847673 0.07002072 0.9170665 ] at iter 90045\n",
      "policy [0.06894103 0.06408795 0.2304437  0.63652736] at iter 95046\n",
      "policy [0.05079623 0.05259403 0.17467105 0.72193867] at iter 100053\n",
      "policy [0.02992335 0.04166204 0.15203522 0.77637935] at iter 105055\n",
      "policy [0.01277913 0.02217139 0.11340012 0.85164934] at iter 110066\n",
      "policy [0.01685992 0.03572369 0.11010049 0.83731586] at iter 115067\n",
      "policy [0.02233979 0.04356821 0.12206813 0.8120239 ] at iter 120068\n",
      "policy [0.0538866  0.06990433 0.19241999 0.6837891 ] at iter 125070\n",
      "policy [0.04998036 0.0586222  0.18841155 0.7029859 ] at iter 130072\n",
      "policy [0.04312687 0.04053227 0.1924797  0.7238611 ] at iter 135074\n",
      "policy [0.04705957 0.04308158 0.23203805 0.67782074] at iter 140076\n",
      "policy [0.05629363 0.03858298 0.31469396 0.5904295 ] at iter 145083\n",
      "policy [0.08654511 0.05349357 0.36883178 0.49112952] at iter 150088\n",
      "policy [0.12689057 0.08578555 0.38690162 0.4004222 ] at iter 155091\n",
      "policy [0.13949919 0.09199794 0.36860842 0.39989445] at iter 160092\n",
      "policy [0.16054647 0.11499221 0.35904956 0.36541173] at iter 165096\n",
      "policy [0.1587208  0.11834364 0.36691684 0.35601878] at iter 170098\n",
      "policy [0.15827143 0.11420427 0.36597225 0.3615521 ] at iter 175100\n",
      "policy [0.15453029 0.11024933 0.3686882  0.36653218] at iter 180102\n",
      "policy [0.16139184 0.10776452 0.37026808 0.3605755 ] at iter 185104\n",
      "policy [0.15280409 0.09590041 0.3848702  0.36642534] at iter 190105\n",
      "policy [0.15396692 0.09676833 0.37935662 0.3699082 ] at iter 195113\n",
      "Current model save name: model_0\n",
      "Current global iteration 199095\n",
      "policy [0.15667252 0.1000023  0.3734843  0.36984086] at iter 200116\n",
      "policy [0.16451563 0.10771615 0.36403352 0.36373466] at iter 205117\n",
      "policy [0.16192205 0.10494217 0.3607454  0.3723904 ] at iter 210120\n",
      "policy [0.16141696 0.10298186 0.35742015 0.378181  ] at iter 215123\n",
      "policy [0.16981268 0.11302825 0.34720117 0.36995786] at iter 220126\n",
      "policy [0.16037737 0.10211332 0.35670748 0.38080186] at iter 225131\n",
      "policy [0.13231592 0.07075914 0.37705138 0.41987354] at iter 230133\n",
      "policy [0.13653275 0.07058024 0.3721876  0.4206994 ] at iter 235135\n",
      "policy [0.12450788 0.06039254 0.38734755 0.42775196] at iter 240140\n",
      "policy [0.11647107 0.05304744 0.39676902 0.4337125 ] at iter 245141\n",
      "policy [0.12588198 0.06049071 0.40275472 0.41087258] at iter 250145\n",
      "policy [0.1291272  0.06444411 0.39960134 0.4068273 ] at iter 255146\n",
      "policy [0.11821185 0.06121417 0.4134188  0.40715522] at iter 260151\n",
      "policy [0.12121899 0.06435291 0.41045964 0.40396845] at iter 265153\n",
      "policy [0.13645141 0.07630166 0.3862242  0.40102273] at iter 270155\n",
      "policy [0.1366351  0.07625999 0.37164235 0.41546255] at iter 275160\n",
      "policy [0.14974959 0.08617406 0.35957044 0.4045059 ] at iter 280162\n",
      "policy [0.14765836 0.07882889 0.35819107 0.41532162] at iter 285165\n",
      "policy [0.15760584 0.08364434 0.3493661  0.4093838 ] at iter 290166\n",
      "policy [0.1484453  0.07457198 0.3465216  0.43046114] at iter 295167\n",
      "policy [0.13256189 0.06128015 0.34915096 0.45700696] at iter 300168\n",
      "policy [0.12356471 0.06057541 0.3667787  0.44908115] at iter 305169\n",
      "policy [0.12565765 0.06249258 0.36024278 0.451607  ] at iter 310174\n",
      "policy [0.1366397  0.07445236 0.34810334 0.44080457] at iter 315175\n",
      "policy [0.13697925 0.07310638 0.3572037  0.43271068] at iter 320180\n",
      "policy [0.1325076  0.0643495  0.3593639  0.44377896] at iter 325184\n",
      "policy [0.1228498  0.05521021 0.37294674 0.44899324] at iter 330185\n",
      "policy [0.13644426 0.06162138 0.38068667 0.42124775] at iter 335186\n",
      "policy [0.14559646 0.0698542  0.3595108  0.42503852] at iter 340188\n",
      "policy [0.1527792  0.07714837 0.3769388  0.3931337 ] at iter 345191\n",
      "policy [0.15903635 0.07994562 0.3707753  0.3902428 ] at iter 350192\n",
      "policy [0.16897987 0.08645497 0.35752916 0.387036  ] at iter 355194\n",
      "policy [0.1761135  0.0920132  0.35305315 0.37882012] at iter 360198\n",
      "policy [0.18677524 0.10693322 0.34602115 0.36027044] at iter 365199\n",
      "policy [0.19458391 0.11621258 0.33036998 0.3588335 ] at iter 370200\n",
      "policy [0.19331492 0.11623518 0.33094433 0.35950556] at iter 375204\n",
      "policy [0.18749523 0.10576569 0.3412     0.36553904] at iter 380205\n",
      "policy [0.18123518 0.09654161 0.35213104 0.37009215] at iter 385206\n",
      "policy [0.18282011 0.09950203 0.35737297 0.36030486] at iter 390207\n",
      "policy [0.17194945 0.08861114 0.36313492 0.37630448] at iter 395211\n",
      "policy [0.16682371 0.0835265  0.36255163 0.3870981 ] at iter 400212\n",
      "policy [0.16079888 0.07942419 0.3683303  0.39144665] at iter 405213\n",
      "policy [0.16413887 0.08501326 0.36934367 0.38150424] at iter 410215\n",
      "Current model save name: model_0\n",
      "Current global iteration 413783\n",
      "policy [0.16381775 0.08849439 0.36875993 0.3789279 ] at iter 415217\n",
      "policy [0.17321359 0.09992692 0.3681049  0.3587546 ] at iter 420219\n",
      "policy [0.17890212 0.10715184 0.36066264 0.35328344] at iter 425220\n",
      "policy [0.1789839  0.10922073 0.3532066  0.35858876] at iter 430221\n",
      "policy [0.17578268 0.10253516 0.3557699  0.36591223] at iter 435222\n",
      "policy [0.17585288 0.10096008 0.35982457 0.3633625 ] at iter 440223\n",
      "policy [0.16639836 0.08468833 0.37534562 0.3735678 ] at iter 445224\n",
      "policy [0.16246289 0.08208913 0.38440946 0.37103847] at iter 450227\n",
      "policy [0.17099848 0.09245518 0.37977007 0.35677627] at iter 455231\n",
      "policy [0.18228206 0.115927   0.36557996 0.33621103] at iter 460234\n",
      "policy [0.187283   0.12062316 0.35694334 0.33515048] at iter 465236\n",
      "policy [0.19021884 0.12170436 0.3471666  0.3409102 ] at iter 470240\n",
      "policy [0.18286331 0.11002304 0.3403824  0.36673123] at iter 475241\n",
      "policy [0.17898202 0.10494851 0.34864715 0.36742234] at iter 480242\n",
      "policy [0.1774388  0.10203227 0.3498264  0.37070256] at iter 485243\n",
      "policy [0.17850544 0.10195582 0.34538716 0.37415162] at iter 490244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy [0.15657529 0.08044509 0.34672964 0.41625   ] at iter 495256\n",
      "policy [0.15268593 0.07261635 0.3291268  0.44557098] at iter 500259\n",
      "policy [0.16681333 0.08628661 0.33166608 0.41523397] at iter 505263\n",
      "policy [0.16926526 0.09389261 0.3379984  0.39884377] at iter 510266\n",
      "policy [0.16209677 0.0967112  0.3487445  0.39244756] at iter 515267\n",
      "policy [0.15289026 0.09529331 0.37391016 0.3779063 ] at iter 520270\n",
      "policy [0.16100667 0.11066786 0.37257448 0.355751  ] at iter 525273\n",
      "policy [0.16405666 0.11267477 0.3643177  0.35895088] at iter 530275\n",
      "policy [0.15641999 0.10200611 0.3628719  0.37870198] at iter 535277\n",
      "policy [0.14826791 0.09057293 0.377398   0.38376114] at iter 540279\n",
      "policy [0.14613912 0.08811335 0.379785   0.38596258] at iter 545280\n",
      "policy [0.16421874 0.0976996  0.35590357 0.38217804] at iter 550282\n",
      "policy [0.16818206 0.10161626 0.35376486 0.3764368 ] at iter 555284\n",
      "policy [0.16009659 0.08837586 0.34709635 0.40443122] at iter 560285\n",
      "policy [0.16150904 0.08892628 0.3415978  0.4079669 ] at iter 565286\n",
      "policy [0.14349319 0.0685818  0.32833076 0.4595943 ] at iter 570287\n",
      "policy [0.13582939 0.06644965 0.33532384 0.46239707] at iter 575292\n",
      "policy [0.13568176 0.06601228 0.32356563 0.4747403 ] at iter 580293\n",
      "policy [0.15428638 0.0728499  0.30865553 0.46420822] at iter 585294\n",
      "policy [0.14973804 0.07136952 0.33420685 0.4446856 ] at iter 590295\n",
      "policy [0.14856055 0.07201502 0.34466055 0.43476388] at iter 595302\n",
      "policy [0.14638358 0.0682414  0.3243593  0.46101567] at iter 600311\n",
      "policy [0.14472508 0.07501245 0.3579178  0.42234474] at iter 605326\n",
      "policy [0.1426646  0.07499568 0.3486665  0.43367317] at iter 610327\n",
      "policy [0.13588838 0.0696997  0.35030466 0.44410732] at iter 615328\n",
      "policy [0.12844981 0.06067092 0.34747773 0.46340156] at iter 620330\n",
      "policy [0.13383663 0.06788043 0.33612546 0.46215752] at iter 625331\n",
      "Current model save name: model_0\n",
      "Current global iteration 627394\n",
      "policy [0.1257271  0.06238507 0.33199155 0.47989634] at iter 630332\n",
      "policy [0.11131748 0.05054102 0.32917422 0.50896734] at iter 635333\n",
      "policy [0.10473882 0.04612781 0.33039492 0.51873845] at iter 640334\n",
      "policy [0.10463327 0.04782477 0.31586    0.5316819 ] at iter 645335\n",
      "policy [0.10991555 0.05412305 0.321043   0.5149184 ] at iter 650338\n",
      "policy [0.13048889 0.06318055 0.31627566 0.49005497] at iter 655339\n",
      "policy [0.13299733 0.06503084 0.31843916 0.48353267] at iter 660340\n",
      "policy [0.13939457 0.06980202 0.31923005 0.4715734 ] at iter 665341\n",
      "policy [0.14817323 0.07884712 0.32811803 0.44486162] at iter 670342\n",
      "policy [0.14863837 0.07980505 0.35525325 0.4163033 ] at iter 675345\n",
      "policy [0.15629545 0.08881097 0.35375816 0.40113533] at iter 680348\n",
      "policy [0.1691574  0.11095242 0.34346074 0.37642947] at iter 685352\n",
      "policy [0.1678681  0.11020905 0.34819102 0.37373182] at iter 690354\n",
      "policy [0.16758882 0.10879795 0.35188693 0.3717263 ] at iter 695357\n",
      "policy [0.16547696 0.11280813 0.34741002 0.37430492] at iter 700359\n",
      "policy [0.15330549 0.12056287 0.33458096 0.3915507 ] at iter 705362\n",
      "policy [0.1544684  0.11702756 0.34232512 0.38617885] at iter 710363\n",
      "policy [0.15597579 0.12122535 0.32999736 0.39280143] at iter 715364\n",
      "policy [0.15160169 0.11265604 0.3258572  0.40988508] at iter 720366\n",
      "policy [0.13741247 0.09667826 0.34225902 0.4236502 ] at iter 725367\n",
      "policy [0.14080144 0.09294429 0.35534784 0.4109064 ] at iter 730368\n",
      "policy [0.13799608 0.09622653 0.34796223 0.41781512] at iter 735372\n",
      "policy [0.12176504 0.09894869 0.35366338 0.42562297] at iter 740378\n",
      "policy [0.11999116 0.09455591 0.3557101  0.42974284] at iter 745381\n",
      "policy [0.11022679 0.09169389 0.3543011  0.44377825] at iter 750382\n",
      "policy [0.12210228 0.10109621 0.37198097 0.40482056] at iter 755383\n",
      "policy [0.13402534 0.103241   0.37768131 0.38505226] at iter 760387\n",
      "policy [0.13542233 0.10663026 0.36300337 0.39494404] at iter 765389\n",
      "policy [0.14234057 0.11251477 0.35675585 0.38838878] at iter 770392\n",
      "policy [0.14647168 0.11537176 0.35810965 0.38004693] at iter 775393\n",
      "policy [0.14682807 0.11046261 0.3675371  0.37517223] at iter 780407\n",
      "policy [0.14824833 0.11197185 0.36981308 0.36996678] at iter 785409\n",
      "policy [0.1436757  0.11096772 0.37502423 0.37033233] at iter 790410\n",
      "policy [0.13542672 0.11878052 0.36992785 0.3758649 ] at iter 795413\n",
      "policy [0.14016996 0.12810682 0.3657347  0.36598852] at iter 800414\n",
      "policy [0.1562545  0.14130671 0.34547368 0.3569651 ] at iter 805424\n",
      "policy [0.17395891 0.14170352 0.33278465 0.3515529 ] at iter 810425\n",
      "policy [0.1753489  0.13973035 0.32979417 0.3551266 ] at iter 815426\n",
      "policy [0.17591499 0.14128333 0.32779625 0.3550055 ] at iter 820428\n",
      "policy [0.17134322 0.13965914 0.3414386  0.34755898] at iter 825429\n",
      "policy [0.17468594 0.14917378 0.33495486 0.34118542] at iter 830430\n",
      "policy [0.17517024 0.15047023 0.3309005  0.34345898] at iter 835431\n",
      "policy [0.17192562 0.13418253 0.3356671  0.35822472] at iter 840432\n",
      "Current model save name: model_0\n",
      "Current global iteration 840630\n",
      "policy [0.18457225 0.14519876 0.3302504  0.3399786 ] at iter 845439\n",
      "policy [0.19039047 0.14305644 0.3368521  0.329701  ] at iter 850440\n",
      "policy [0.18862279 0.14243951 0.33679575 0.33214197] at iter 855442\n",
      "policy [0.18655293 0.13938577 0.3351056  0.3389557 ] at iter 860448\n",
      "policy [0.18133052 0.13865584 0.34436628 0.3356474 ] at iter 865450\n",
      "policy [0.18539818 0.13777272 0.34651077 0.3303183 ] at iter 870452\n",
      "policy [0.18157244 0.14170319 0.34626603 0.3304584 ] at iter 875455\n",
      "policy [0.17700861 0.15101567 0.34408277 0.32789293] at iter 880462\n",
      "policy [0.17990948 0.15767282 0.33313513 0.32928258] at iter 885464\n",
      "policy [0.17282075 0.15120399 0.33790696 0.3380683 ] at iter 890466\n",
      "policy [0.18040103 0.15608759 0.33034596 0.33316538] at iter 895468\n",
      "policy [0.18195051 0.15433541 0.33349356 0.33022046] at iter 900469\n",
      "policy [0.18544719 0.1479994  0.34115508 0.32539827] at iter 905471\n",
      "policy [0.18684335 0.14992116 0.3356303  0.32760516] at iter 910474\n",
      "policy [0.18468341 0.15205517 0.3328492  0.33041227] at iter 915476\n",
      "policy [0.19912492 0.15313268 0.31446752 0.33327484] at iter 920478\n",
      "policy [0.20759714 0.15547262 0.310347   0.32658327] at iter 925479\n",
      "policy [0.19901542 0.13456778 0.333821   0.33259585] at iter 930481\n",
      "policy [0.19246952 0.13036056 0.34980628 0.3273636 ] at iter 935482\n",
      "policy [0.19459926 0.14042933 0.34837762 0.31659386] at iter 940484\n",
      "policy [0.20149723 0.15622011 0.33614156 0.30614117] at iter 945485\n",
      "policy [0.20341435 0.16349779 0.33117095 0.3019169 ] at iter 950486\n",
      "policy [0.20993325 0.15521583 0.3334379  0.301413  ] at iter 955487\n",
      "policy [0.21029542 0.15516663 0.33327025 0.3012677 ] at iter 960496\n",
      "policy [0.20655352 0.1545728  0.33040664 0.30846706] at iter 965497\n",
      "policy [0.20199908 0.15318663 0.33143374 0.31338048] at iter 970498\n",
      "policy [0.20745732 0.15500303 0.32845494 0.30908468] at iter 975499\n",
      "policy [0.2033767  0.1497317  0.3322736  0.31461805] at iter 980500\n",
      "policy [0.20322001 0.14878574 0.33249208 0.31550208] at iter 985501\n",
      "policy [0.20265897 0.14274047 0.33936316 0.31523746] at iter 990508\n",
      "policy [0.20675777 0.14585015 0.3325822  0.31480986] at iter 995513\n",
      "policy [0.20173328 0.14110544 0.33776924 0.31939206] at iter 1000515\n",
      "policy [0.19937314 0.14087841 0.34206104 0.3176874 ] at iter 1005517\n",
      "policy [0.20419493 0.14370483 0.334221   0.3178792 ] at iter 1010518\n",
      "policy [0.20442246 0.14323364 0.3338893  0.31845465] at iter 1015525\n",
      "policy [0.2075181  0.14191657 0.3250538  0.32551152] at iter 1020526\n",
      "policy [0.20033728 0.14309087 0.3136447  0.34292716] at iter 1025528\n",
      "policy [0.19231918 0.14352392 0.31214586 0.35201108] at iter 1030529\n",
      "policy [0.19219014 0.1442518  0.3105669  0.35299113] at iter 1035530\n",
      "policy [0.19622353 0.1492626  0.30697608 0.34753776] at iter 1040536\n",
      "policy [0.19056039 0.14578682 0.31294543 0.35070735] at iter 1045543\n",
      "policy [0.18876126 0.13696578 0.31735942 0.3569135 ] at iter 1050549\n",
      "Current model save name: model_0\n",
      "Current global iteration 1054889\n",
      "policy [0.18707806 0.13184294 0.32104117 0.36003777] at iter 1055550\n",
      "policy [0.18597916 0.13412715 0.31434363 0.36555   ] at iter 1060551\n",
      "policy [0.18622614 0.14604425 0.30473417 0.36299542] at iter 1065552\n",
      "policy [0.18552078 0.1455661  0.30456403 0.36434913] at iter 1070553\n",
      "policy [0.16588417 0.11760037 0.3040431  0.4124723 ] at iter 1075554\n",
      "policy [0.16264232 0.11302892 0.30093876 0.42339003] at iter 1080555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy [0.15859158 0.10127153 0.2903766  0.44976026] at iter 1085561\n",
      "policy [0.17760988 0.09358468 0.2825505  0.44625497] at iter 1090571\n",
      "policy [0.1962799  0.09800624 0.2809186  0.42479527] at iter 1095572\n",
      "policy [0.18926774 0.0943426  0.31287274 0.40351695] at iter 1100573\n",
      "policy [0.17701508 0.1003195  0.33059454 0.39207092] at iter 1105574\n",
      "policy [0.19020727 0.11077015 0.31204066 0.38698193] at iter 1110575\n",
      "policy [0.1890851  0.10999441 0.3134802  0.3874403 ] at iter 1115581\n",
      "policy [0.19106826 0.1154066  0.3200422  0.37348288] at iter 1120585\n",
      "policy [0.1923377  0.12497652 0.31574017 0.3669456 ] at iter 1125589\n",
      "policy [0.19842733 0.13103014 0.31373495 0.3568076 ] at iter 1130592\n",
      "policy [0.20479298 0.13381436 0.31090596 0.35048673] at iter 1135595\n",
      "policy [0.20880572 0.15463743 0.30423823 0.33231863] at iter 1140605\n",
      "policy [0.2082017  0.15630278 0.3045028  0.3309927 ] at iter 1145606\n",
      "policy [0.20003445 0.1461394  0.3109261  0.3429    ] at iter 1150608\n",
      "policy [0.19718829 0.14111207 0.30842295 0.3532767 ] at iter 1155609\n",
      "policy [0.20030874 0.15297206 0.30598584 0.34073335] at iter 1160610\n",
      "policy [0.1962824  0.15827781 0.3050575  0.3403823 ] at iter 1165611\n",
      "policy [0.19302264 0.15645833 0.308628   0.34189105] at iter 1170612\n",
      "policy [0.19295943 0.15439425 0.31062123 0.34202507] at iter 1175615\n",
      "policy [0.18817528 0.14320572 0.31105474 0.35756427] at iter 1180616\n",
      "policy [0.18446173 0.146135   0.3108601  0.35854322] at iter 1185617\n",
      "policy [0.17978577 0.14793478 0.3124077  0.3598718 ] at iter 1190620\n",
      "policy [0.17796442 0.14977488 0.3082055  0.36405525] at iter 1195621\n",
      "policy [0.17301346 0.14869747 0.30612877 0.37216032] at iter 1200624\n",
      "policy [0.16862701 0.13459834 0.31027427 0.3865004 ] at iter 1205625\n",
      "policy [0.17449903 0.12624924 0.31421548 0.38503626] at iter 1210629\n",
      "policy [0.17617998 0.12532187 0.31689057 0.3816076 ] at iter 1215634\n",
      "policy [0.17965494 0.13949203 0.32345974 0.35739332] at iter 1220638\n",
      "policy [0.1873799  0.14948077 0.31465814 0.3484812 ] at iter 1225639\n",
      "policy [0.18686005 0.15703136 0.3096431  0.34646553] at iter 1230645\n",
      "policy [0.18315542 0.15204711 0.311473   0.3533244 ] at iter 1235653\n",
      "policy [0.18369552 0.14353374 0.30795783 0.36481294] at iter 1240656\n",
      "policy [0.1834349  0.14254929 0.30844146 0.3655744 ] at iter 1245659\n",
      "policy [0.17687157 0.12713325 0.32167897 0.37431625] at iter 1250665\n",
      "policy [0.17953551 0.12862115 0.31825393 0.3735895 ] at iter 1255668\n",
      "policy [0.18650384 0.1466766  0.30329204 0.36352754] at iter 1260669\n",
      "policy [0.1919981  0.14550494 0.30084923 0.36164767] at iter 1265676\n",
      "Current model save name: model_0\n",
      "Current global iteration 1268987\n",
      "policy [0.19944908 0.14547247 0.2980044  0.35707405] at iter 1270678\n",
      "policy [0.2085545  0.14468367 0.29252613 0.35423565] at iter 1275680\n",
      "policy [0.2120932  0.14736569 0.2941187  0.34642243] at iter 1280682\n",
      "policy [0.20426905 0.14326678 0.2968165  0.35564768] at iter 1285693\n",
      "policy [0.20148431 0.1355981  0.29963496 0.36328268] at iter 1290703\n",
      "policy [0.20173433 0.13778898 0.29898778 0.3614889 ] at iter 1295705\n",
      "policy [0.19706191 0.12857284 0.30061832 0.3737469 ] at iter 1300706\n",
      "policy [0.20221674 0.12866427 0.29568455 0.37343445] at iter 1305708\n",
      "policy [0.20532428 0.12956534 0.29094023 0.37417015] at iter 1310709\n",
      "policy [0.20217487 0.13416277 0.28975227 0.3739101 ] at iter 1315710\n",
      "policy [0.20078674 0.13345219 0.2965633  0.36919773] at iter 1320715\n",
      "policy [0.19641553 0.13126744 0.301677   0.37064   ] at iter 1325716\n",
      "policy [0.1843532  0.13313666 0.31864733 0.3638628 ] at iter 1330717\n",
      "policy [0.18655252 0.14013483 0.31477162 0.35854095] at iter 1335721\n",
      "policy [0.18782558 0.14419402 0.31161478 0.35636565] at iter 1340723\n",
      "policy [0.19252533 0.14609328 0.3139133  0.34746805] at iter 1345725\n",
      "policy [0.19856808 0.14510989 0.30969885 0.34662324] at iter 1350730\n",
      "policy [0.19989777 0.15366644 0.30435458 0.3420812 ] at iter 1355731\n",
      "policy [0.19548242 0.15418673 0.3099788  0.34035212] at iter 1360732\n",
      "policy [0.1961233  0.15209545 0.3108145  0.34096676] at iter 1365736\n",
      "policy [0.20585246 0.15719931 0.30505064 0.3318976 ] at iter 1370740\n",
      "policy [0.21053897 0.16550344 0.3000057  0.32395187] at iter 1375751\n",
      "policy [0.21721958 0.1702728  0.2987359  0.31377178] at iter 1380752\n",
      "policy [0.21319017 0.15869324 0.30114922 0.3269674 ] at iter 1385755\n",
      "policy [0.21310315 0.16227725 0.29708815 0.3275314 ] at iter 1390757\n",
      "policy [0.22294183 0.17042999 0.28529254 0.32133564] at iter 1395758\n",
      "policy [0.22335355 0.1695871  0.28559023 0.32146907] at iter 1400762\n",
      "policy [0.23088613 0.16808142 0.28311828 0.3179142 ] at iter 1405763\n",
      "policy [0.23332012 0.1666671  0.28097662 0.31903616] at iter 1410770\n",
      "policy [0.23054974 0.1641726  0.28339675 0.32188088] at iter 1415774\n",
      "policy [0.23181812 0.1660089  0.28397125 0.3182017 ] at iter 1420775\n",
      "policy [0.23233654 0.16549726 0.28395778 0.3182084 ] at iter 1425776\n",
      "policy [0.22786164 0.16338453 0.28551647 0.32323733] at iter 1430779\n",
      "policy [0.228306   0.16846482 0.28046224 0.32276687] at iter 1435780\n",
      "policy [0.23009045 0.17270263 0.276156   0.32105085] at iter 1440781\n",
      "policy [0.23329082 0.18755126 0.26872554 0.31043234] at iter 1445783\n",
      "policy [0.23242234 0.19068575 0.27029032 0.30660158] at iter 1450785\n",
      "policy [0.23481126 0.19273314 0.267588   0.30486763] at iter 1455787\n",
      "policy [0.23152001 0.19040468 0.27429706 0.30377823] at iter 1460791\n",
      "policy [0.23176801 0.19178039 0.2718188  0.30463287] at iter 1465793\n",
      "policy [0.23335148 0.18447843 0.26772934 0.31444073] at iter 1470795\n",
      "policy [0.23183511 0.1777113  0.26916358 0.32128996] at iter 1475798\n",
      "policy [0.22734153 0.17205523 0.27246544 0.3281378 ] at iter 1480801\n",
      "Current model save name: model_0\n",
      "Current global iteration 1483152\n",
      "policy [0.2194768  0.1632543  0.2715781  0.34569076] at iter 1485802\n",
      "policy [0.22411178 0.17421113 0.26681557 0.33486152] at iter 1490806\n",
      "policy [0.2265461  0.18193093 0.2623748  0.32914814] at iter 1495811\n",
      "policy [0.22238763 0.18508585 0.26652572 0.32600078] at iter 1500813\n",
      "policy [0.22074565 0.18391791 0.2604245  0.3349119 ] at iter 1505814\n",
      "policy [0.2184794  0.18394488 0.26332548 0.33425027] at iter 1510817\n",
      "policy [0.2177466  0.18289278 0.26372775 0.33563286] at iter 1515822\n",
      "policy [0.21750996 0.17979264 0.26172742 0.34097004] at iter 1520823\n",
      "policy [0.21622762 0.17837155 0.26429862 0.34110224] at iter 1525827\n",
      "policy [0.21495305 0.17389719 0.2693494  0.3418004 ] at iter 1530830\n",
      "policy [0.21017861 0.16950533 0.2724582  0.34785783] at iter 1535831\n"
     ]
    }
   ],
   "source": [
    "cnt_threads = 20\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "def worker_fun(worker, sess, optimizer, thread_lock):\n",
    "    worker.work(sess, optimizer, thread_lock)\n",
    "\n",
    "for i in range(cnt_threads):\n",
    "    env = EnvWrapper(ENV_NAME)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE, decay=DECAY)\n",
    "    worker = Worker(Agent(env, 'local' + str(i), optimizer))\n",
    "    t = threading.Thread(target=worker_fun, args=(worker, sess, optimizer, thread_lock))\n",
    "    worker_threads.append(t)\n",
    "    time.sleep(0.15)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in worker_threads:\n",
    "    t.start()\n",
    "    time.sleep(0.15)\n",
    "    \n",
    "global_t = threading.Thread(target=global_saving_thread, args=(global_agent, sess))\n",
    "\n",
    "worker_threads.append(global_t)\n",
    "global_t.start()\n",
    "\n",
    "for t in worker_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning time was\", int(elapsed_time/60/60), \"hours\", int((elapsed_time - int(elapsed_time/60/60)*60*60)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_fun(test_agent):\n",
    "    test_env = EnvWrapper(ENV_NAME)\n",
    "    #test_agent = Agent(test_env, 'tester', optimizer)\n",
    "    test_agent.update_to_global(sess)\n",
    "\n",
    "    done = False\n",
    "    state = test_env.reset()\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        policy = sess.run((test_agent.policy), \\\n",
    "                                                feed_dict={\\\n",
    "                                                           test_agent.X:state\\\n",
    "                                                          }\\\n",
    "                                               )\n",
    "        policy = policy.flatten()\n",
    "        #print('cur policy', policy)\n",
    "        #prediction = np.argmax(policy)\n",
    "        prediction = np.random.choice(test_env.action_space.n, p=policy)\n",
    "        if random.random() < 0.05:\n",
    "            prediction = env.action_space.sample()\n",
    "\n",
    "        ns, r, d, _ = test_env.step(prediction)\n",
    "        test_env.env.render()\n",
    "        state = ns\n",
    "        reward += (r)\n",
    "        done = d\n",
    "    test_env.env.close()\n",
    "    print('final reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = EnvWrapper(ENV_NAME)\n",
    "tester_agent = global_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_fun(tester_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
